<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">

<head>

<meta charset="utf-8">
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="pandoc" />



<meta name="progressive" content="false" />
<meta name="allow-skip" content="false" />

<title>PHS Summer Prep</title>


<!-- highlightjs -->
<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
  pre:not([class]) {
    background-color: white;
  }
</style>
<script type="text/javascript">
if (window.hljs) {
  hljs.configure({languages: []});
  hljs.initHighlightingOnLoad();
  if (document.readyState && document.readyState === "complete") {
    window.setTimeout(function() { hljs.initHighlighting(); }, 0);
  }
}
</script>

<!-- taken from https://github.com/rstudio/rmarkdown/blob/67b7f5fc779e4cfdfd0f021d3d7745b6b6e17149/inst/rmd/h/default.html#L296-L362 -->
<!-- tabsets -->

<style type="text/css">
.tabset-dropdown > .nav-tabs {
  display: inline-table;
  max-height: 500px;
  min-height: 44px;
  overflow-y: auto;
  background: white;
  border: 1px solid #ddd;
  border-radius: 4px;
}

.tabset-dropdown > .nav-tabs > li.active:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li.active:before {
  content: "&#xe258;";
  border: none;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs > li.active {
  display: block;
}

.tabset-dropdown > .nav-tabs > li > a,
.tabset-dropdown > .nav-tabs > li > a:focus,
.tabset-dropdown > .nav-tabs > li > a:hover {
  border: none;
  display: inline-block;
  border-radius: 4px;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li {
  display: block;
  float: none;
}

.tabset-dropdown > .nav-tabs > li {
  display: none;
}
</style>

<script>
$(document).ready(function () {
  window.buildTabsets("section-TOC");
});

$(document).ready(function () {
  $('.tabset-dropdown > .nav-tabs > li').click(function () {
    $(this).parent().toggleClass('nav-tabs-open')
  });
});
</script>
<!-- end tabsets -->


<link rel="stylesheet" href="www/style.css" type="text/css" />

</head>

<body>



<div class="pageContent band">
<div class="bandContent page">

<div class="topics">

<p><div id="keep_alive" class="shiny-text-output"></div></p>
<div id="section-probability-density-functions" class="section level2">
<h2>Probability density functions</h2>
<div id="section-continuous-probability-distributions" class="section level4">
<h4>Continuous probability distributions</h4>
<p>With <strong>discrete distributions</strong>, we defined the probability mass function (pmf) as <span class="math inline">\(f(x) = P(X = x)\)</span>. However, we have left the corresponding function for <strong>continuous distributions</strong> a mystery… until now.</p>
<p>A continuous random variable can take on <em>any</em> value within a given interval. Unlike discrete distributions, for which we could list the possible values (even if they go off to infinity, like the Poisson distribution, we can imagine that we could just keep counting and counting…), the possible values of a continuous distribution aren’t countable. For example, consider the uniform(0, 1) distribution, which has equal probability over the interval between 0 and 1. What’s the first value that comes after 0?</p>
<p>That’s of course not a question that you can answer, though it may provide some intuition to the following statement: for a continuous random variable, the <em>probability of any given value is 0</em>. That is, <span class="math inline">\(P(X = x) = 0\)</span> for all <span class="math inline">\(x\)</span>. If you are drawing from the Poisson(2) distribution we keep looking at, some of the time you’ll get values of 0 or 1, for example. But if you are drawing from a uniform(0, 1) distribution, the probability that you’ll get exactly 0.5 is 0.</p>
<p>Because of this, we need a function that differs from the pmf: the <strong>probability density function</strong>, or pdf. We can’t define the pdf the same way we defined the pmf for a discrete variable. We’ll still refer to it as <span class="math inline">\(f(x)\)</span>, but its values aren’t probabilities, they’re “densities”. Unlike a probability, a probability <em>density</em> can have values that are greater than 1! You can think of them as relative probabilities. Values of <span class="math inline">\(x\)</span> with larger densities are more likely than those with smaller densities.</p>
<p>Let’s look at the pdf of the <strong>standard normal distribution</strong>. This is our first continuous probability distribution: the normal, or Gaussian, distribution. This is the one with the famous bell-curve shape.</p>
<p>A normally distributed random variable <span class="math inline">\(X \sim N(\mu, \sigma^2)\)</span> is defined by two <strong>parameters</strong>: its mean (<span class="math inline">\(\mu\)</span>) and its variance (<span class="math inline">\(\sigma^2\)</span>).</p>
<p>What does that mean? The <span class="math inline">\(\sim\)</span> symbol tells us how the random variable <span class="math inline">\(X\)</span> is distributed (you can read <span class="math inline">\(\sim\)</span> as “is distributed”). In this case, it follows a normal (abbreviated <span class="math inline">\(N\)</span>) distribution. The values in parentheses are the parameters that characterize that distribution. The normal distribution has two, but we saw earlier that a Bernoulli distributed random variable is defined by one parameter, its probability <span class="math inline">\(p\)</span>, as is a Poisson distributed random variable, which has a rate parameter <span class="math inline">\(\lambda\)</span>. We could therefore write <span class="math inline">\(Y \sim Bernoulli(p)\)</span> and <span class="math inline">\(Z \sim Poisson(\lambda)\)</span> for random variables <span class="math inline">\(Y\)</span> and <span class="math inline">\(Z\)</span> that follow those respective distributions.</p>
<p>The standard normal is has mean 0 and variance 1: we could write it normal(0, 1). Here’s its pdf: <img src="index_files/figure-html/normpdf-1.png" width="624" style="display: block; margin: auto;" /></p>
<div class="panel panel-default">
<div data-label="mostLikely" class="tutorial-question panel-body">
<div id="mostLikely-answer_container" class="shiny-html-output"></div>
<div id="mostLikely-message_container" class="shiny-html-output"></div>
<div id="mostLikely-action_button_container" class="shiny-html-output"></div>
<script>if (Tutorial.triggerMathJax) Tutorial.triggerMathJax()</script>
</div>
</div>
<div class="panel panel-default">
<div data-label="mostLikely2" class="tutorial-question panel-body">
<div id="mostLikely2-answer_container" class="shiny-html-output"></div>
<div id="mostLikely2-message_container" class="shiny-html-output"></div>
<div id="mostLikely2-action_button_container" class="shiny-html-output"></div>
<script>if (Tutorial.triggerMathJax) Tutorial.triggerMathJax()</script>
</div>
</div>
<p>Although the values along the y-axis aren’t probabilities, one important thing you should know about pdfs is that no matter the scale on the x-axis or the magnitude of the density values on the y-axis, the function will <em>always integrate to 1</em>. That is, <span class="math inline">\(\int_{-\infty}^{\infty} f(x) \, dx = 1\)</span>. That’s because the total probability of all the possible values of the distribution has to be 1 – the exact same reason our pmfs for discrete distributions summed to 1.</p>
</div>
<div id="section-return-to-the-cdf" class="section level4">
<h4>Return to the cdf</h4>
<p>We can define the pdf in terms of the cumulative distribution function (cdf), or <span class="math inline">\(F(x)\)</span>. Before we do that, let’s review cdfs, and in particular, see that we can apply them to continuous distributions.</p>
<p>Remember that the cdf is defined as <span class="math inline">\(F(x) = P(X \leq x)\)</span>. This holds true for both continuous and discrete random variables.</p>
<p>Also remember what we said about the whole pdf integrating to 1: the area under the curve represents the total probability. So if we want to know, say, <span class="math inline">\(F(1) = P(X \leq 1)\)</span> we can just integrate over the pdf from <span class="math inline">\(-\infty\)</span> to <span class="math inline">\(1\)</span>: <span class="math display">\[F(1) = \int_{-\infty}^1 f(x) \,dx \; \text{, where } f(x) \text{ is the standard normal pdf.}\]</span></p>
<p><img src="index_files/figure-html/normpdfCol-1.png" width="624" style="display: block; margin: auto;" /></p>
<p>It turns out that the blue area, <span class="math inline">\(F(1) \approx 0.84134\)</span>. That’s the probability that a standard normal random variable is <span class="math inline">\(\leq 1\)</span>. We’ll come back to this number in a bit.</p>
</div>
<div id="section-intuiting-the-cdf-and-its-relationship-with-the-pdf" class="section level4">
<h4>Intuiting the cdf and its relationship with the pdf</h4>
<p>Take a second to think about what the function <span class="math inline">\(F(x)\)</span> looks like for the standard normal, based on the picture of its pdf. (Try to draw it out!) Think about how the integral changes as you increase the values of <span class="math inline">\(x\)</span>. How does <span class="math inline">\(F(-2)\)</span> compare to <span class="math inline">\(F(-1)\)</span> and <span class="math inline">\(F(0)\)</span>? What are the limiting values as <span class="math inline">\(x\)</span> approaches <span class="math inline">\(- \infty\)</span> and <span class="math inline">\(\infty\)</span>? Make sure your drawing of <span class="math inline">\(F(x)\)</span> matches the correct answers to the following questions, which you should know from the introduction to cdfs in the context of discrete distributions.</p>
<div class="panel panel-default">
<div data-label="Fquiz" class="tutorial-question panel-body">
<div id="Fquiz-answer_container" class="shiny-html-output"></div>
<div id="Fquiz-message_container" class="shiny-html-output"></div>
<div id="Fquiz-action_button_container" class="shiny-html-output"></div>
<script>if (Tutorial.triggerMathJax) Tutorial.triggerMathJax()</script>
</div>
</div>
<div class="panel panel-default">
<div data-label="Fquiz2" class="tutorial-question panel-body">
<div id="Fquiz2-answer_container" class="shiny-html-output"></div>
<div id="Fquiz2-message_container" class="shiny-html-output"></div>
<div id="Fquiz2-action_button_container" class="shiny-html-output"></div>
<script>if (Tutorial.triggerMathJax) Tutorial.triggerMathJax()</script>
</div>
</div>
<div class="panel panel-default">
<div data-label="Fquiz3" class="tutorial-question panel-body">
<div id="Fquiz3-answer_container" class="shiny-html-output"></div>
<div id="Fquiz3-message_container" class="shiny-html-output"></div>
<div id="Fquiz3-action_button_container" class="shiny-html-output"></div>
<script>if (Tutorial.triggerMathJax) Tutorial.triggerMathJax()</script>
</div>
</div>
<p>Does this look like what you drew? This is the standard normal cdf:</p>
<p><img src="index_files/figure-html/cdfpic-1.png" width="624" style="display: block; margin: auto;" /></p>
<p>We have defined the cdf as <span class="math inline">\(P(X \leq x)\)</span>, and we have some idea about how it’s related to the pdf via an integral. We still don’t actually have a definition for the pdf, though!</p>
<p>Remember something super important from calculus? Like, say, its Fundamental Theorem? It basically told us we could go back and forth between derivatives and integrals. We have the cdf in terms of the integral of the pdf, so let’s just go backwards and define the pdf in terms of the derivative of the cdf.</p>
<p><span class="math display">\[f(x) = F&#39;(x) = \frac{d}{dx}F(x)\]</span></p>
<p>What is this telling us? If we evaluate the derivative of the cdf at any point <span class="math inline">\(x\)</span>, we get the density at that point, or <span class="math inline">\(f(x)\)</span>.</p>
<p>Is this intuitive? Look back at the graph of the standard normal cdf.</p>
<div class="panel panel-default">
<div data-label="Fxq" class="tutorial-question panel-body">
<div id="Fxq-answer_container" class="shiny-html-output"></div>
<div id="Fxq-message_container" class="shiny-html-output"></div>
<div id="Fxq-action_button_container" class="shiny-html-output"></div>
<script>if (Tutorial.triggerMathJax) Tutorial.triggerMathJax()</script>
</div>
</div>
<p>The slope of the function <span class="math inline">\(F(x)\)</span> is steepest at 0. That means its derivative, and therefore <span class="math inline">\(f(x)\)</span>, is the largest at 0. Does this match what we saw in the graph of <span class="math inline">\(f(x)\)</span> above?</p>
<p>What about when <span class="math inline">\(F(x)\)</span> is the flattest, and <span class="math inline">\(f(x)\)</span> the smallest – that is, around -4 or 4? (If this graph could go from <span class="math inline">\(-\infty\)</span> to <span class="math inline">\(\infty\)</span> it would, those are just arbitrary endpoints.) Well, <span class="math inline">\(P(X \leq -4)\)</span> is not much different from <span class="math inline">\(P(X \leq -3.75)\)</span>, and <span class="math inline">\(P(X \leq 4)\)</span> is not much different from <span class="math inline">\(P(X \leq 4.25)\)</span> because there’s not a lot of chance that the random variable will take on values in those intervals. That matches what we saw in the pdf: the density, or relative probability, between each pair of those values is very low. Around 0, however, the density is high, so <span class="math inline">\(P(X \leq -0.25)\)</span> is very different from <span class="math inline">\(P(X \leq 0)\)</span> is very different from <span class="math inline">\(P(X \leq 0.25)\)</span>.</p>
</div>
<div id="section-other-examples-of-distributions" class="section level4">
<h4>Other examples of distributions</h4>
<p>For each of the following three distributions, compare the cdf to the pdf. Find the x-values with the highest densities and note that they correspond to the places on the cdf with the largest slope. Confirm that each pdf and cdf adheres to the rules that we’ve laid out for them. <img src="index_files/figure-html/expCompare-1.png" width="624" style="display: block; margin: auto;" /></p>
<p><img src="index_files/figure-html/chiCompare-1.png" width="624" style="display: block; margin: auto;" /></p>
<p><img src="index_files/figure-html/betaCompare-1.png" width="624" style="display: block; margin: auto;" /></p>
</div>
<div id="section-the-normal-distribution-in-r" class="section level4">
<h4>The normal distribution in R</h4>
<p>It turns out that the pdf for the normal distribution is defined by: <span class="math display">\[f(x) = \frac{1}{\sqrt{2\pi\sigma^2}}\exp\left(-\frac{1}{2}\left(\frac{x - \mu}{\sigma}\right)^2\right)\]</span> so we can just integrate that function from <span class="math inline">\(-\infty\)</span> to 1 with <span class="math inline">\(\mu = 0\)</span> and <span class="math inline">\(\sigma^2 = 1\)</span> to get <span class="math inline">\(F(1)\)</span>, the value that we saw above was equal to about 0.84. We don’t expect you to do that yourselves, though! That’s what R is for.</p>
<p>Remember how we used functions for the cdfs of discrete random variables using the prefix <code>p</code> earlier? We can do the same with continuous distributions. The “shortcut” for the normal distribution is <code>norm</code>, and the set of functions take the arguments <code>mean =</code> and <code>sd =</code> (note the use of <span class="math inline">\(\sigma\)</span> as an argument, although we usually refer to the variance parameter <span class="math inline">\(\sigma^2\)</span> when we’re talking about the normal distribution). It turns out that the default values of <code>mean=</code> and <code>sd=</code> are 0 and 1, respectively, so if you want the standard normal, you don’t need to specify the parameters.</p>
<p>See if you can find <span class="math inline">\(F(1)\)</span> for the standard normal.</p>
<div class="tutorial-exercise" data-label="F1" data-caption="Code" data-completion="1" data-diagnostics="1" data-startover="1" data-lines="1">
<script type="application/json" data-opts-chunk="1">{"fig.width":6.5,"fig.height":4,"fig.retina":2,"fig.align":"center","fig.keep":"high","fig.show":"asis","out.width":624,"warning":true,"error":false,"message":true,"exercise.df_print":"paged","exercise.timelimit":60,"exercise.checker":"NULL"}</script>
</div>
<div class="tutorial-exercise-support" data-label="F1-solution" data-caption="Code" data-completion="1" data-diagnostics="1" data-startover="1" data-lines="0">
<pre class="text"><code>pnorm(1)</code></pre>
</div>
<p>Let’s think about what this value means for a bit.</p>
<p>If we were able to draw an infinite number of samples from a standard normal distribution, then about 84% of them would be &lt; 1. When are we ever drawing infinite samples from a standard normal distribution though? If you said never, you’re absolutely right. Nonetheless, this and other values from the standard normal cdf are still incredibly helpful and you will end up using them all the time!</p>
Let’s first see what it would look like if we could draw a ton (though not an infinite number!) of realizations from a standard normal, which R allows us to do. The prefix R, when used with the distributional abbreviations, gives random (really pseudo-random) draws from that distribution. Let’s save 10,000 of them to the vector <code>x</code>. Try using the functions you know on that vector. Don’t print out the whole vector, though, because it’s huge! You can also try to figure out other functions which may be useful – often they’re very common-sense. Try <code>sd()</code> or <code>min()</code>.
<div class="tutorial-exercise-support" data-label="norm-prep" data-caption="Code" data-completion="1" data-diagnostics="1" data-startover="1" data-lines="0">
<pre class="text"><code>set.seed(123)
x &lt;- rnorm(10000)</code></pre>
</div>
(Note the line <code>set.seed(123)</code> below. This gives R a certain random number to start at so that we can all get the same values on our computers. Whenever you’re running code with a <code>set.seed()</code> command, make sure to re-run that line every time you re-run the code, so your results match up. If you’re doing your own simulation, pick a random integer as an argument for the function – don’t always use <code>123</code>!)
<div class="tutorial-exercise" data-label="xnorm" data-caption="Code" data-completion="1" data-diagnostics="1" data-startover="1" data-lines="4">
<pre class="text"><code>set.seed(123)
x &lt;- rnorm(10000)</code></pre>
<script type="application/json" data-opts-chunk="1">{"fig.width":6.5,"fig.height":4,"fig.retina":2,"fig.align":"center","fig.keep":"high","fig.show":"asis","out.width":624,"warning":true,"error":false,"message":true,"exercise.df_print":"paged","exercise.timelimit":60,"exercise.setup":"norm-prep","exercise.checker":"NULL"}</script>
</div>
<p>Let’s first try to recreate the pdf for the standard normal using these data. Remember that since we are in the continuous case, <span class="math inline">\(f(x) \neq P(X = x)\)</span> for <span class="math inline">\(X \sim Normal(0, 1)\)</span>. To see this in action, consider <span class="math inline">\(P(X = 0)\)</span>. This is the mean of the distribution – and the height of the bell shape, so we might expect the largest value if we run <code>mean(x == 0)</code> (recall that’s how we can estimate a probability from that data).</p>
<div class="tutorial-exercise" data-label="dens" data-caption="Code" data-completion="1" data-diagnostics="1" data-startover="1" data-lines="1">
<pre class="text"><code>mean(x == 0)</code></pre>
<script type="application/json" data-opts-chunk="1">{"fig.width":6.5,"fig.height":4,"fig.retina":2,"fig.align":"center","fig.keep":"high","fig.show":"asis","out.width":624,"warning":true,"error":false,"message":true,"exercise.df_print":"paged","exercise.timelimit":60,"exercise.setup":"norm-prep","exercise.checker":"NULL"}</script>
</div>
<p>You were certainly expecting this if you looked at the data, perhaps with <code>head(x)</code>. Of course none of them would be exactly 0.</p>
<p>Now see if you can estimate <span class="math inline">\(F(1)\)</span> using these data.</p>
<div class="tutorial-exercise" data-label="propLT1" data-caption="Code" data-completion="1" data-diagnostics="1" data-startover="1" data-lines="2">
<script type="application/json" data-opts-chunk="1">{"fig.width":6.5,"fig.height":4,"fig.retina":2,"fig.align":"center","fig.keep":"high","fig.show":"asis","out.width":624,"warning":true,"error":false,"message":true,"exercise.df_print":"paged","exercise.timelimit":60,"exercise.setup":"norm-prep","exercise.checker":"NULL"}</script>
</div>
<div class="tutorial-exercise-support" data-label="propLT1-hint-1" data-caption="Code" data-completion="1" data-diagnostics="1" data-startover="1" data-lines="0">
<pre class="text"><code>mean(...) # what logical statement should you use?</code></pre>
</div>
<div class="tutorial-exercise-support" data-label="propLT1-hint-2" data-caption="Code" data-completion="1" data-diagnostics="1" data-startover="1" data-lines="0">
<pre class="text"><code>mean(x &lt;= 1)</code></pre>
</div>
<p>You didn’t get exactly <span class="math inline">\(0.84134\)</span>, because we are working with a finite sample, so we can only <strong>estimate</strong> <span class="math inline">\(F(1)\)</span>, with some error. It should be close, though! (That’s because of the law of large numbers, which tells us that sample means approach expected values as <span class="math inline">\(n\)</span> approaches <span class="math inline">\(\infty\)</span>. We’ll look at expected values in the next section.)</p>
<p>Finally, let’s see if the values we drew in match the standard normal cdf in general, and not just at <span class="math inline">\(F(1)\)</span>. Instead of checking every possible value between <span class="math inline">\(-\infty\)</span> and <span class="math inline">\(\infty\)</span> (impossible!) we can use the <code>ecdf()</code> (empirical cumulative distribution function) command. We’ll then plot a dashed red line with the true values on top to compare (don’t worry about the code here, but see if you can decode it, perhaps using the help files as a guide).</p>
<div class="tutorial-exercise" data-label="ecdf" data-caption="Code" data-completion="1" data-diagnostics="1" data-startover="1" data-lines="2">
<pre class="text"><code>plot(ecdf(x))
curve(pnorm(x), add = TRUE, col = &quot;red&quot;, lty = &quot;dashed&quot;)</code></pre>
<script type="application/json" data-opts-chunk="1">{"fig.width":6.5,"fig.height":4,"fig.retina":2,"fig.align":"center","fig.keep":"high","fig.show":"asis","out.width":624,"warning":true,"error":false,"message":true,"exercise.df_print":"paged","exercise.timelimit":60,"exercise.setup":"norm-prep","exercise.checker":"NULL"}</script>
</div>
<p>Looks great!</p>
</div>
<div id="section-more-resources" class="section level4">
<h4>More resources</h4>
<p>Learn more about continuous random variables and probability density functions <a href="https://www.khanacademy.org/math/statistics-probability/random-variables-stats-library/random-variables-continuous/v/probability-density-functions">here</a>.</p>
</div>
</div>
<div id="section-expectations-and-variance" class="section level2">
<h2>Expectations and variance</h2>
<p>We’ve already seen a couple of ways we can describe distributions: <span class="math inline">\(F(x)\)</span> and <span class="math inline">\(f(x)\)</span>. In particular, in the last section we saw how those are related for continuous distributions. However, we need some other tools to help us characterize distributions, particularly when we want to compare them.</p>
<div id="section-expectations" class="section level4">
<h4>Expectations</h4>
<p>The expected value, or expectation, <span class="math inline">\(E[X]\)</span> tells us the average value of the random variable <span class="math inline">\(X\)</span>: its mean.</p>
<p>Expectations are calculated by summing or integrating <span class="math inline">\(x \times f(x)\)</span> over the real line. (We can sum for discrete random variables but have to integrate for continuous random variables.) <span class="math display">\[\text{E}[X] = \sum_{x \in \mathcal{X}} x\, f(x)\]</span></p>
<p><span class="math display">\[\text{E}[X] = \int_{-\infty}^{\infty} x\, f(x)\, dx\]</span> Remember that <span class="math inline">\(f(X)\)</span> is like a relative probability for continuous random variables (and of course, the actual probabilities <span class="math inline">\(P(X = x)\)</span> for discrete variables). So what these expressions are saying is that to find the average value of the random variable <span class="math inline">\(X\)</span>, we have to weight all the values it can take on by their relative probabilities. (In the summation notation, the <span class="math inline">\(x \in \mathcal{X}\)</span> just means that we sum over all the possible values <span class="math inline">\(X\)</span> can take on.)</p>
<p>Think about the Normal(<span class="math inline">\(0, 1\)</span>) distribution. The normal distribution has support over the entire real line, which means that the probability <span class="math inline">\(P(X \leq -2,534,623,623) \neq 0\)</span>, and <span class="math inline">\(P(X &gt; 8,134,235,214) \neq 0\)</span>, to take some arbitrary values. However, the density <span class="math inline">\(f(x)\)</span> evaluated at those is so, so small that it’s basically 0. In other words, it’s <em>incredibly</em> unlikely that we’d ever draw values around those numbers from an N(<span class="math inline">\(0, 1\)</span>) distribution (think about the maximum and minimum values from your standard normal sample in the last section – were they anywhere near that?). That means when we integrate over extreme values like those, they get basically no weight.</p>
<p>We saw that the standard normal density is highest around 0. You can think of the integral as sweeping over all the possible values: when it gets to those around 0, such as -0.5 or 0.25, it weights them higher, and it weights those the closest to 0 the most. Because in the case of the normal, <span class="math inline">\(f(x)\)</span> is symmetric, negative values get weighted equally to their positive counterparts, so <span class="math inline">\(E[X] = 0\)</span>. Of course! The 0 in <span class="math inline">\(N(0, 1)\)</span> already told us that its mean is 0!</p>
<p>If we have a N(<span class="math inline">\(3, 4\)</span>) distribution, it’s still symmetric, but now symmetric around another value: <img src="index_files/figure-html/otherNorm-1.png" width="624" style="display: block; margin: auto;" /></p>
<p>Now the weights that values around 2 get are the same as those around 4, since <span class="math inline">\(f(2) = f(4)\)</span>. Same with <span class="math inline">\(0\)</span> and <span class="math inline">\(6\)</span>. See how this is going to average out to give us an expected value of <span class="math inline">\(3\)</span>? We learned something about the central location of the distribution from its expectation.</p>
</div>
<div id="section-calculating-an-expectation" class="section level4">
<h4>Calculating an expectation</h4>
<p>OK, you don’t have to integrate anything. But how about some summations for discrete distributions?</p>
<p>The Bernoulli(.25) distribution has pmf: <span class="math display">\[f(x) = \begin{cases} 
      0.75 &amp; \text{for } x =  0 \\
      0.25 &amp; \text{for } x =  1 
   \end{cases}
\]</span> So <span class="math display">\[\begin{equation*}
\begin{split}
E[X] &amp; = \sum_{x = 0}^1 x\,f(x) \\
&amp; = 0\times 0.75 + 1 \times 0.25\\
&amp; = 0.25
\end{split}
\end{equation*}\]</span></p>
<p>This, of course, matches what we already know about the Bernoulli distribution: its mean is its probability parameter. We could have replaced <span class="math inline">\(0.25\)</span> with <span class="math inline">\(p\)</span> and <span class="math inline">\(0.75\)</span> with <span class="math inline">\(1 - p\)</span> and found <span class="math inline">\(E[X] = p\)</span>.</p>
<p>Now look at this distribution: <span class="math display">\[f(x) = \begin{cases} 
      0.25 &amp; \text{for } x =  -4 \\
      0.5 &amp; \text{for } x =  1 \\
      0.25 &amp; \text{for } x =  19 \\
   \end{cases}
\]</span></p>
<div class="panel panel-default">
<div data-label="expPractice" class="tutorial-question panel-body">
<div id="expPractice-answer_container" class="shiny-html-output"></div>
<div id="expPractice-message_container" class="shiny-html-output"></div>
<div id="expPractice-action_button_container" class="shiny-html-output"></div>
<script>if (Tutorial.triggerMathJax) Tutorial.triggerMathJax()</script>
</div>
</div>
<p>Did you do something like this? <span class="math display">\[\begin{equation*}
\begin{split}
E[X] &amp; = \sum_{x} x\,f(x) \\
&amp; = -4\times 0.25 + 1 \times 0.5 + 19 \times 0.25\\
&amp; = 4.25
\end{split}
\end{equation*}\]</span></p>
<p>Note that in both cases, the expected value of the distribution is not a value the random variable itself can have. A binary variable can of course never have the value 0.25, and in the second distribution, <span class="math inline">\(X\)</span> can only take on three values, none of which is its expectation. This is an important point to remember when we start modeling, because we are often modeling expectations and not the actual observed or observable values of variables.</p>
</div>
<div id="section-another-way-to-describe-distributions" class="section level4">
<h4>Another way to describe distributions</h4>
<p>We can learn something about the spread of a distribution from its <strong>variance</strong>: <span class="math display">\[Var(X) = E[(X - E[X])^2] = \int_{-\infty}^{\infty} (x - E[X])^2\, f(x) dx \;, \]</span><br />
where <span class="math inline">\(E[X]\)</span> is the expectation we’ve already looked at – the mean (again, we can sum instead of integrate for discrete variables).</p>
<p>Notice how we’re doing the exact same thing we did above to calculate the variance, which is the <strong>expectation</strong> of a <em>function</em> of <span class="math inline">\(X\)</span>. We’re integrating (or summing) that function, multiplied by <span class="math inline">\(f(x)\)</span>, over all the possible values that <span class="math inline">\(X\)</span> can take on. The function of <span class="math inline">\(X\)</span> that we’re interested in is <span class="math inline">\((X - E[X])^2\)</span>.</p>
<p>Think of it this way: we want to know, on average (so we integrate over everything, weighted by its relative probability), how far away a random variable might be from its mean (so we find the distance by subtracting). We square it because we don’t care about which direction the distance is in: we want 3 less than the mean to be worth the same as 3 more than the mean. (In fact, if we didn’t square it, we’d just get 0… can you figure out why?)</p>
<p>Compare these distributions: <img src="index_files/figure-html/compareNorms-1.png" width="624" style="display: block; margin: auto;" /></p>
<p>All of the above distributions have mean 0, so <span class="math inline">\(Var(X) = E[(X - E[X])^2] = E[(X - 0)^2]\)</span>. Look at a value like 5. All four distributions have non-zero density at <span class="math inline">\(X = 5\)</span>, so when we integrate over 5, that value will make <em>some</em> contribution to the variance. And since they all have the same mean, the value <span class="math inline">\((X - E[X])^2 = (5 - 0)^2\)</span> is the same. However, <span class="math inline">\(f(5)\)</span> is much larger for the green line than the red line. So a large value like <span class="math inline">\((5 - 0)^2 = 25\)</span> will contribute more to the variance for the green distribution. Small values close to 0 still contribute the most for each of the distributions, but they contribute relatively less for the green one, compared to more extreme values. The shape of the green distribution is broader. That’s why its variance is much higher.</p>
</div>
<div id="section-practice-with-variances" class="section level4">
<h4>Practice with variances</h4>
<p>Let’s look back at the distribution above and calculate its variance. We already calculated <span class="math inline">\(E[X] = 4.25\)</span>, so we can plug that into our expression. Again, we are summing over the possible values and weighting by the probabilities of those values, but this time what we are summing is <span class="math inline">\((x - 4.25)^2\)</span>.</p>
<p>As a reminder, the pmf was <span class="math display">\[f(x) = \begin{cases} 
      0.25 &amp; \text{for } x =  -4 \\
      0.5 &amp; \text{for } x =  1 \\
      0.25 &amp; \text{for } x =  19 \\
   \end{cases}
\]</span> so we can calculate the variance as follows: <span class="math display">\[\begin{equation*}
\begin{split}
Var(X) &amp; = \sum_{x} (x  - E[X])^2\,f(x) \\
&amp; = (-4 - 4.25)^2\times 0.25 + (1 - 4.25)^2 \times 0.5 + (19 - 4.25)^2 \times 0.25\\
&amp; = 76.6875
\end{split}
\end{equation*}\]</span></p>
<p>Now you try to calculate the variance of the Bernoulli(<span class="math inline">\(0.25\)</span>) distribution, which we already calculated the mean of (so you can plug that in for <span class="math inline">\(E[X]\)</span>).</p>
<div class="panel panel-default">
<div data-label="varPractice" class="tutorial-question panel-body">
<div id="varPractice-answer_container" class="shiny-html-output"></div>
<div id="varPractice-message_container" class="shiny-html-output"></div>
<div id="varPractice-action_button_container" class="shiny-html-output"></div>
<script>if (Tutorial.triggerMathJax) Tutorial.triggerMathJax()</script>
</div>
</div>
<p>Instead of going through all the math, there are usually closed-form expressions for variances of distributions in terms of their parameters, just like we found that the mean of a Bernoulli is just <span class="math inline">\(p\)</span>. Let’s look at in general how we would find <span class="math inline">\(Var(X)\)</span> if <span class="math inline">\(X \sim Bernoulli(p)\)</span>.</p>
<p>The Bernoulli(<span class="math inline">\(p\)</span>) distribution has pmf: <span class="math display">\[f(x) = \begin{cases} 
      1 - p &amp; \text{for } x =  0 \\
      p &amp; \text{for } x =  1 
   \end{cases}
\]</span> so <span class="math display">\[\begin{equation*}
\begin{split}
E[(X - E[X])^2] &amp; = \sum_{x = 0}^1 (x - E[X])^2\,f(x) \\
&amp; = \sum_{x = 0}^1 (x - p)^2\,p^x(1 - p)^{1 - x}\\
&amp; = p^2(1 - p) + (1-p)^2p \\
&amp; = p^2 - p^3 + p - 2p^2 + p^3 \\
&amp; = p(1 - p)
\end{split}
\end{equation*}\]</span></p>
<p>This is one of those things you should just memorize, because we use a lot of binary variables in the population health sciences, and each one of them has variance <span class="math inline">\(p(1-p)\)</span>!</p>
</div>
<div id="section-covariance" class="section level4">
<h4>Covariance</h4>
<p>Like the name implies, covariance tells us the extent to which two variables vary together, or tend to move in the same direction. If two random variables are independent, knowing about one tells you nothing about the other, so their <strong>covariance</strong> is zero. Let’s break down its definition: <span class="math display">\[Cov(X, Y) = E[(X - E[X])(Y - E[Y])]\]</span> First of all, note that if we replace <span class="math inline">\(Y\)</span> with <span class="math inline">\(X\)</span> – so look at the covariance of <span class="math inline">\(X\)</span> with itself – we are back to the definition of variance. The covariance just extends the same concept to another variable. When <span class="math inline">\(X\)</span> is far from its mean at the same time <span class="math inline">\(Y\)</span> is from <em>its</em> mean, then the covariance will be large. This implies that they are moving together, away from their respective means at the same time (and of course close to their respective means at the same time). If the two variables are independent, then, on average, the probability that <span class="math inline">\(X\)</span> is above its mean and <span class="math inline">\(Y\)</span> is above its mean is balanced out by the probability that <span class="math inline">\(X\)</span> is above its mean and <span class="math inline">\(Y\)</span> is <em>below</em> its mean (and vice versa), since they happen equally often if <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> have no relationship.</p>
<p>On average, when someone is heavier than the mean weight in the population, they will also be taller than the mean height. When someone is shorter, then also tend to be lighter. Since both variables tend away from the mean in the same direction, their covariance is positive. However, if we are looking at the relationship between animal size and sleep duration, we will see a different relationship. Animals like elephants and giraffes tend to be bigger than the average animal, and they only sleep a few hours a day. The animals that sleep more than average, like bats and chipmunks, are the smallest ones. This inverse relationship means that the covariance between those two variables is negative – on average, when one is above the mean, the other is below, and vice versa.</p>
</div>
<div id="section-properties-of-the-mean-and-variance" class="section level4">
<h4>Properties of the mean and variance</h4>
<p>There are also some simple rules for means and variances – of any distribution – that you should memorize. In each case take <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span> to be scalars; that is, a single real number, while <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are random variables: <span class="math display">\[E[aX] = aE[X]\]</span> <span class="math display">\[E[X + Y] = E[X] + E[Y]\]</span> <span class="math display">\[E[aX + bY] = aE[X] + bE[Y]\]</span> <span class="math display">\[Var(aX) = a^2Var(X)\]</span> <span class="math display">\[Var(X + Y) = Var(X) + Var(Y) + 2Cov(X, Y)\]</span> <span class="math display">\[Var(aX + bY) = a^2Var(X) + b^2Var(Y) + 2abCov(X, Y)\]</span> <span class="math display">\[Var(X) = E[(X - E[X])^2] = E[X^2] - (E[X])^2\]</span></p>
<p>Use this code box to explore these rules. I’ll start you off with the first one. You already have seen <code>mean()</code>; you’ll also want to use the functions <code>var()</code>, and <code>cov()</code>. Note that even though we have been mostly using the normal distribution for simplicity, these rules are true of any distribution, so play around! I’ll use the exponential, which is another continuous distribution, but you can try any others (remember they each have their own arguments; the exponential takes <code>rate =</code>). When looking at two random variables <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>, you can even use two different distributions.</p>
<div class="tutorial-exercise" data-label="rules" data-caption="Code" data-completion="1" data-diagnostics="1" data-startover="1" data-lines="6">
<pre class="text"><code>X &lt;- rexp(n = 1000, rate = 4)
a &lt;- 92
mean(a * X)
a * mean(X)</code></pre>
<script type="application/json" data-opts-chunk="1">{"fig.width":6.5,"fig.height":4,"fig.retina":2,"fig.align":"center","fig.keep":"high","fig.show":"asis","out.width":624,"warning":true,"error":false,"message":true,"exercise.df_print":"paged","exercise.timelimit":60,"exercise.checker":"NULL"}</script>
</div>
<p>Of course, we are showing these rules with sample means and variances, and not the true parameters. If you drew from a Bernoulli distribution, you probably noticed that the mean of your sample wasn’t exactly <span class="math inline">\(p\)</span> and the variance not exactly <span class="math inline">\(p(1 - p)\)</span>. In a large enough sample, however, the sample values should be quite close to the true parameters. We’ll learn more about this in the modeling and estimation section.</p>
</div>
<div id="section-conditional-expectations" class="section level4">
<h4>Conditional expectations</h4>
<p>Like probabilities, we can also have <strong>conditional</strong> expectations, which we can write like this: <span class="math inline">\(E[X | Y = y]\)</span>. We are still curious about the average value of the random variable <span class="math inline">\(X\)</span>, but now we only want to know its mean when <span class="math inline">\(Y\)</span> takes on a certain value.</p>
<p>Previously we dichotomized age (<span class="math inline">\(A &lt; 50\)</span>, <span class="math inline">\(A \geq 50\)</span>) when we looked at its relationship with disease status. While in some situations it may be appropriate to consider all people older than 50 “old”, and all those younger “young”, and only compare those two large strata, in other situations we might be concerned with the average age, and not the proportion above 50. We might, for example, want to compare <span class="math inline">\(E[A | D = 1]\)</span> and <span class="math inline">\(E[A | D = 0]\)</span>, the expected value of age in the diseased and non-diseased, respectively. In a sample, of course, we could just split the data into the two disease groups and calculate the sample mean within each one to estimate those quantities.</p>
<p>In many situations, however, we want to condition on multiple variables. For example, we might want to compare the average age among the diseased and non-diseased, but only among people who test positive for disease. In other words, we want to “hold constant” test status. This is perhaps not the most realistic example, so let’s move to something new. Instead let’s say we are interested in age at onset of puberty. In particular, we may want to know if birthweight is associated with the timing of onset. However, we don’t want to compare boys and girls – we know they’re not directly comparable in both birthweight and age at puberty – so we would like to condition on child sex as well.</p>
<p>Let <span class="math inline">\(A\)</span> = age at onset of puberty, <span class="math inline">\(B\)</span> = birthweight in grams, and <span class="math inline">\(G = 1\)</span> if a child is a girl and <span class="math inline">\(0\)</span> otherwise. See if you can translate the following difference in conditional expectations into a research question: <span class="math display">\[E[A | B = b + 100, G = g] - E[A | B = b, G = g]\]</span> We want to know, on average, what the difference is in age at onset of puberty associated with a 100g difference in birthweight (i.e., from <span class="math inline">\(b\)</span> to <span class="math inline">\(b + 100\)</span>), holding child sex constant (at <span class="math inline">\(g\)</span>). If you have interpreted parameters from regression models before, you may find this language familiar. Notice, though, that we have no model, and we haven’t said anything about how we’re estimating this quantity: we’ve just defined what we’re interested in as a difference in conditional expectations. We’ll get to the estimation later.</p>
</div>
<div id="section-factor-variables-in-r" class="section level4">
<h4>Factor variables in R</h4>
<p>There are quite a few datasets built into R that are often used in examples in the help files or for teaching. We’ll play around with one of those datasets now. To load the <code>iris</code> dataset, use the function <code>data(iris)</code>. Then use the functions you know to explore the dataframe and the variables that make it up before moving on. You can also run <code>?iris</code> to learn more about the data.</p>
<div class="tutorial-exercise" data-label="loadIris" data-caption="Code" data-completion="1" data-diagnostics="1" data-startover="1" data-lines="5">
<script type="application/json" data-opts-chunk="1">{"fig.width":6.5,"fig.height":4,"fig.retina":2,"fig.align":"center","fig.keep":"high","fig.show":"asis","out.width":624,"warning":true,"error":false,"message":true,"exercise.df_print":"paged","exercise.timelimit":60,"exercise.checker":"NULL"}</script>
</div>
<p>You may have noticed that while four of the variables are numeric, one is not: <code>iris$Species</code>. Use the <code>class()</code> function on this variable:</p>
<div class="tutorial-exercise" data-label="irisClass" data-caption="Code" data-completion="1" data-diagnostics="1" data-startover="1" data-lines="1">
<script type="application/json" data-opts-chunk="1">{"fig.width":6.5,"fig.height":4,"fig.retina":2,"fig.align":"center","fig.keep":"high","fig.show":"asis","out.width":624,"warning":true,"error":false,"message":true,"exercise.df_print":"paged","exercise.timelimit":60,"exercise.checker":"NULL"}</script>
</div>
<p>This is a “factor”, or categorical variable. Factor variables are not as straightforward to use as numeric or logical variables, but they are a very useful data type when doing statistics, because R automatically does some of the annoying work for you.</p>
<p>Factor variables have “levels”. Although their levels are usually words, they don’t act like character variables. Instead, each one of the levels is secretly an integer, starting with 1. We’re going to use a tiny subset of the data called <code>iris_sub</code> to explore this further.</p>
<div class="tutorial-exercise-support" data-label="iris-prep" data-caption="Code" data-completion="1" data-diagnostics="1" data-startover="1" data-lines="0">
<pre class="text"><code>iris_sub &lt;- iris[c(1, 58, 34, 111, 89),]
fruits &lt;- as.factor(c(&quot;apple&quot;, &quot;grape&quot;, &quot;grape&quot;, &quot;orange&quot;, &quot;grape&quot;, &quot;apple&quot;))</code></pre>
</div>
Look at the output from these three lines of code. See if you can figure out what’s going on.
<div class="tutorial-exercise" data-label="irisInt" data-caption="Code" data-completion="1" data-diagnostics="1" data-startover="1" data-lines="3">
<pre class="text"><code>iris_sub$Species
as.numeric(iris_sub$Species)
levels(iris_sub$Species)</code></pre>
<script type="application/json" data-opts-chunk="1">{"fig.width":6.5,"fig.height":4,"fig.retina":2,"fig.align":"center","fig.keep":"high","fig.show":"asis","out.width":624,"warning":true,"error":false,"message":true,"exercise.df_print":"paged","exercise.timelimit":60,"exercise.setup":"iris-prep","exercise.checker":"NULL"}</script>
</div>
The <code>levels()</code> function extracts the levels from a factor variable like <code>Species</code>. The levels are printed in order, so that the first listed is 1, the second 2, and so on. We can see that the factor labels are really just hiding the integer values by using the function <code>as.numeric()</code>, which, among other things, can turn a factor variable into a numeric variable. Note that this only works because the variable is of that type – it wouldn’t work on a character vector, as you can see below.
<div class="tutorial-exercise" data-label="irisChar" data-caption="Code" data-completion="1" data-diagnostics="1" data-startover="1" data-lines="1">
<pre class="text"><code>as.numeric(c(&quot;setosa&quot;, &quot;versicolor&quot;, &quot;setosa&quot;, &quot;virginica&quot;, &quot;versicolor&quot;))</code></pre>
<script type="application/json" data-opts-chunk="1">{"fig.width":6.5,"fig.height":4,"fig.retina":2,"fig.align":"center","fig.keep":"high","fig.show":"asis","out.width":624,"warning":true,"error":false,"message":true,"exercise.df_print":"paged","exercise.timelimit":60,"exercise.checker":"NULL"}</script>
</div>
<p>However, we can convert a character vector into a factor. R will generally do this automatically when you read in data, (unless you use the argument <code>stringsAsFactors = FALSE</code>, but you’ll get a chance to practice reading in data once class starts), which is usually helpful, unless you have a variable that you want to remain in character format – say, the names of all the people in the class.</p>
<div class="tutorial-exercise" data-label="charFact" data-caption="Code" data-completion="1" data-diagnostics="1" data-startover="1" data-lines="2">
<pre class="text"><code>fruits &lt;- as.factor(c(&quot;apple&quot;, &quot;grape&quot;, &quot;grape&quot;, &quot;orange&quot;, &quot;grape&quot;, &quot;apple&quot;))
fruits</code></pre>
<script type="application/json" data-opts-chunk="1">{"fig.width":6.5,"fig.height":4,"fig.retina":2,"fig.align":"center","fig.keep":"high","fig.show":"asis","out.width":624,"warning":true,"error":false,"message":true,"exercise.df_print":"paged","exercise.timelimit":60,"exercise.setup":"iris-prep","exercise.checker":"NULL"}</script>
</div>
<p>Notice that when you create a factor from a character vector as above, the levels are automatically created in alphabetical order. The first level (in this case, “apple”) will be the reference level for any comparisons made with the other levels. Often, however, we want to reorder the levels, and in particular choose a new reference level, or change the names of the labels for the levels.</p>
<div class="tutorial-exercise" data-label="fact2" data-caption="Code" data-completion="1" data-diagnostics="1" data-startover="1" data-lines="2">
<pre class="text"><code>fruits2 &lt;- factor(fruits, levels = c(&quot;orange&quot;, &quot;grape&quot;, &quot;apple&quot;)) #reorder
fruits2</code></pre>
<script type="application/json" data-opts-chunk="1">{"fig.width":6.5,"fig.height":4,"fig.retina":2,"fig.align":"center","fig.keep":"high","fig.show":"asis","out.width":624,"warning":true,"error":false,"message":true,"exercise.df_print":"paged","exercise.timelimit":60,"exercise.setup":"iris-prep","exercise.checker":"NULL"}</script>
</div>
<div class="tutorial-exercise" data-label="fact3" data-caption="Code" data-completion="1" data-diagnostics="1" data-startover="1" data-lines="2">
<pre class="text"><code>fruits3 &lt;- relevel(fruits, ref = &quot;orange&quot;) # just choose new reference
fruits3</code></pre>
<script type="application/json" data-opts-chunk="1">{"fig.width":6.5,"fig.height":4,"fig.retina":2,"fig.align":"center","fig.keep":"high","fig.show":"asis","out.width":624,"warning":true,"error":false,"message":true,"exercise.df_print":"paged","exercise.timelimit":60,"exercise.setup":"iris-prep","exercise.checker":"NULL"}</script>
</div>
<div class="tutorial-exercise" data-label="fact4" data-caption="Code" data-completion="1" data-diagnostics="1" data-startover="1" data-lines="2">
<pre class="text"><code>fruits4 &lt;- factor(fruits, labels = c(&quot;APPLE&quot;, &quot;GRAPE&quot;, &quot;ORANGE&quot;)) # relabel
fruits4</code></pre>
<script type="application/json" data-opts-chunk="1">{"fig.width":6.5,"fig.height":4,"fig.retina":2,"fig.align":"center","fig.keep":"high","fig.show":"asis","out.width":624,"warning":true,"error":false,"message":true,"exercise.df_print":"paged","exercise.timelimit":60,"exercise.setup":"iris-prep","exercise.checker":"NULL"}</script>
</div>
<p>Notice that in each case, the observed values retain the same values (though perhaps not the integers behind them) and are in the same order, it’s the levels that have changed.</p>
Returning to the <code>iris_sub</code> dataset, create another variable <code>new_species</code> in the same dataset which is also a factor variable, but which has levels “VIRGINICA”, “VERSICOLOR”, and “SETOSA”, in that order. Print out that variable as well as the dataset to make sure that you didn’t change the observations.
<div class="tutorial-exercise" data-label="newSpec" data-caption="Code" data-completion="1" data-diagnostics="1" data-startover="1" data-lines="3">
<script type="application/json" data-opts-chunk="1">{"fig.width":6.5,"fig.height":4,"fig.retina":2,"fig.align":"center","fig.keep":"high","fig.show":"asis","out.width":624,"warning":true,"error":false,"message":true,"exercise.df_print":"paged","exercise.timelimit":60,"exercise.setup":"iris-prep","exercise.checker":"NULL"}</script>
</div>
<div class="tutorial-exercise-support" data-label="newSpec-hint-1" data-caption="Code" data-completion="1" data-diagnostics="1" data-startover="1" data-lines="0">
<pre class="text"><code>iris_sub$new_species &lt;- # code here</code></pre>
</div>
<div class="tutorial-exercise-support" data-label="newSpec-hint-2" data-caption="Code" data-completion="1" data-diagnostics="1" data-startover="1" data-lines="0">
<pre class="text"><code>iris_sub$new_species &lt;- factor(iris_sub$Species, ...)</code></pre>
</div>
<div class="tutorial-exercise-support" data-label="newSpec-hint-3" data-caption="Code" data-completion="1" data-diagnostics="1" data-startover="1" data-lines="0">
<pre class="text"><code>iris_sub$new_species &lt;- factor(iris_sub$Species, levels = c(&quot;virginica&quot;, &quot;versicolor&quot;, &quot;setosa&quot;), labels = c(&quot;VIRGINICA&quot;, &quot;VERSICOLOR&quot;, &quot;SETOSA&quot;))
iris_sub$new_species
iris_sub</code></pre>
</div>
Now let’s practice estimating some conditional expectations in the entire <code>iris</code> dataset. Previously we estimated conditional probabilities by creating new datasets using the <code>subset()</code> function. Let’s review that here. Calculate the mean petal length among the versicolor species.
<div class="tutorial-exercise" data-label="versSub" data-caption="Code" data-completion="1" data-diagnostics="1" data-startover="1" data-lines="2">
<script type="application/json" data-opts-chunk="1">{"fig.width":6.5,"fig.height":4,"fig.retina":2,"fig.align":"center","fig.keep":"high","fig.show":"asis","out.width":624,"warning":true,"error":false,"message":true,"exercise.df_print":"paged","exercise.timelimit":60,"exercise.checker":"NULL"}</script>
</div>
<div class="tutorial-exercise-support" data-label="versSub-solution" data-caption="Code" data-completion="1" data-diagnostics="1" data-startover="1" data-lines="0">
<pre class="text"><code>versicolor &lt;- subset(iris, subset = Species == &quot;setosa&quot;)
mean(versicolor$Petal.Length)</code></pre>
</div>
<p>There are a number of ways to manipulate and summarize data in R. Some people use the functions in base R, others prefer packages like <code>dplyr</code> and <code>data.table</code>. Becoming fluent in one or more of these strategies will help you immensely with data management and calculating descriptive statistics. We’re just using base R functions in this tutorial, but you can learn more about <code>dplyr</code> <a href="https://dplyr.tidyverse.org">here</a> and <code>data.table</code> <a href="https://cran.r-project.org/web/packages/data.table/vignettes/datatable-intro.html">here</a>.</p>
To quickly calculate the conditional means of petal length for each species, we can use the <code>aggregate()</code> function. The first argument is the variable we are trying to summarize using a function whose name we supply to the <code>FUN =</code> argument. The <code>by =</code> argument must be a list (a type of object we haven’t covered yet, but you can easily create one with the function <code>list()</code>) of the factors you want to group by.
<div class="tutorial-exercise" data-label="aggIris" data-caption="Code" data-completion="1" data-diagnostics="1" data-startover="1" data-lines="1">
<pre class="text"><code>aggregate(iris$Petal.Length, by = list(iris$Species), FUN = mean)</code></pre>
<script type="application/json" data-opts-chunk="1">{"fig.width":6.5,"fig.height":4,"fig.retina":2,"fig.align":"center","fig.keep":"high","fig.show":"asis","out.width":624,"warning":true,"error":false,"message":true,"exercise.df_print":"paged","exercise.timelimit":60,"exercise.checker":"NULL"}</script>
</div>
<p>Use the same function to find the standard deviation of the petal width within each species.</p>
<div class="tutorial-exercise" data-label="aggSD" data-caption="Code" data-completion="1" data-diagnostics="1" data-startover="1" data-lines="1">
<script type="application/json" data-opts-chunk="1">{"fig.width":6.5,"fig.height":4,"fig.retina":2,"fig.align":"center","fig.keep":"high","fig.show":"asis","out.width":624,"warning":true,"error":false,"message":true,"exercise.df_print":"paged","exercise.timelimit":60,"exercise.checker":"NULL"}</script>
</div>
<div class="tutorial-exercise-support" data-label="aggSD-solution" data-caption="Code" data-completion="1" data-diagnostics="1" data-startover="1" data-lines="0">
<pre class="text"><code>aggregate(iris$Petal.Width, by = list(iris$Species), FUN = sd)</code></pre>
</div>
<p>Now let’s try to put all of these ideas together. Another dataset in R is called <code>mtcars</code>. In this dataset, according to the help file, the <code>am</code> variable contains information on type of transmission (0 = automatic, 1 = manual) and <code>vs</code> on the type of engine (0 = V-shaped, 1 = straight). Create two new variables, <code>transmission</code> and <code>engine</code>, with this same data in the form of factors with appropriate labels. Make “manual” and “straight” the reference levels. Then calculate the conditional means of the <code>mpg</code> variable for each <em>combination</em> of transmission and engine type using the <code>aggregate()</code> function.</p>
<div class="tutorial-exercise" data-label="aggFin" data-caption="Code" data-completion="1" data-diagnostics="1" data-startover="1" data-lines="5">
<script type="application/json" data-opts-chunk="1">{"fig.width":6.5,"fig.height":4,"fig.retina":2,"fig.align":"center","fig.keep":"high","fig.show":"asis","out.width":624,"warning":true,"error":false,"message":true,"exercise.df_print":"paged","exercise.timelimit":60,"exercise.checker":"NULL"}</script>
</div>
<div class="tutorial-exercise-support" data-label="aggFin-hint-1" data-caption="Code" data-completion="1" data-diagnostics="1" data-startover="1" data-lines="0">
<pre class="text"><code>mtcars$transmission &lt;- factor(...)</code></pre>
</div>
<div class="tutorial-exercise-support" data-label="aggFin-hint-2" data-caption="Code" data-completion="1" data-diagnostics="1" data-startover="1" data-lines="0">
<pre class="text"><code>mtcars$transmission &lt;- factor(mtcars$am, levels = c(&quot;1&quot;, &quot;0&quot;), labels = c(&quot;manual&quot;, &quot;automatic&quot;))</code></pre>
</div>
<div class="tutorial-exercise-support" data-label="aggFin-hint-3" data-caption="Code" data-completion="1" data-diagnostics="1" data-startover="1" data-lines="0">
<pre class="text"><code>aggregate(mtcars$mpg, by = list(..., ...), FUN = mean)</code></pre>
</div>
<div class="tutorial-exercise-support" data-label="aggFin-hint-4" data-caption="Code" data-completion="1" data-diagnostics="1" data-startover="1" data-lines="0">
<pre class="text"><code>mtcars$transmission &lt;- factor(mtcars$am, levels = c(&quot;1&quot;, &quot;0&quot;), labels = c(&quot;manual&quot;, &quot;automatic&quot;))
mtcars$engine &lt;- factor(mtcars$vs, levels = c(&quot;1&quot;, &quot;0&quot;), labels = c(&quot;straight&quot;, &quot;V-shaped&quot;))
aggregate(mtcars$mpg, by = list(mtcars$transmission, mtcars$engine), FUN = mean)</code></pre>
</div>
</div>
<div id="section-more-resources-1" class="section level4">
<h4>More resources</h4>
<p>Start <a href="https://www.khanacademy.org/math/statistics-probability/random-variables-stats-library/random-variables-discrete/v/expected-value-of-a-discrete-random-variable">here</a> for information about expectations, <a href="https://www.khanacademy.org/math/statistics-probability/random-variables-stats-library/random-variables-discrete/v/variance-and-standard-deviation-of-a-discrete-random-variable">here</a> for variance. The examples <a href="https://www.khanacademy.org/math/statistics-probability/random-variables-stats-library/binomial-mean-standard-dev-formulas/v/mean-and-variance-of-bernoulli-distribution-example">here</a> for the Bernoulli and binomial distributions are very helpful.</p>
</div>
</div>
<div id="section-matrix-notation-and-multiplication" class="section level2">
<h2>Matrix notation and multiplication</h2>
<p>We often use matrix notation in statistics because it is much simpler when we are working with multiple variables. Matrix multiplication allows us to easily compute sums and products across a number of variables. Let’s review the rules, but first let’s define the types of objects we’re working with.</p>
<p><span class="math display">\[a = a \text{ , a scalar}\]</span></p>
<p><span class="math display">\[
\mathbf{b} = \begin{bmatrix}
b_1 \\ b_2 \\ \vdots \\ b_p
\end{bmatrix} \text{ , a vector of length $p$}
\]</span></p>
<p><span class="math display">\[
\mathbf{X} = \begin{bmatrix}
x_{11} &amp; x_{12} &amp; \cdots &amp; x_{1p} \\ 
x_{21} &amp; x_{22} &amp; \cdots &amp; x_{2p} \\ 
\vdots &amp;\vdots &amp; \ddots &amp; \vdots \\ 
x_{n1} &amp; x_{n2} &amp; \cdots &amp; x_{np}
\end{bmatrix} \text{ , an $n \times p$ matrix}
\]</span></p>
<p>A scalar is just a single value. A vector is a column of values (e.g., observations of random variables or estimates of parameters) and can also be considered a <span class="math inline">\(p \times 1\)</span> matrix. Note that when we label the dimensions of a matrix or index its values, we do it in the order <span class="math inline">\(rows \times columns\)</span>. This is also the order in which matrices are indexed in R, so make sure you memorize it. Vectors and matrices are often written in bold to make it clear they are not scalars.</p>
<p>When we multiply a scalar by a vector or a matrix, we can just do so element by element:</p>
<p><span class="math display">\[a\mathbf{b} = \begin{bmatrix}
ab_1 \\ ab_2 \\ \vdots \\ ab_p
\end{bmatrix}
\hspace{1cm} 
a\mathbf{X} = \begin{bmatrix}
ax_{11} &amp; ax_{12} &amp; \cdots &amp; ax_{1p} \\ 
ax_{21} &amp; ax_{22} &amp; \cdots &amp; ax_{2p} \\ 
\vdots &amp;\vdots &amp; \ddots &amp; \vdots \\ 
ax_{n1} &amp; ax_{n2} &amp; \cdots &amp; ax_{np}
\end{bmatrix}
\]</span></p>
<p>However, when we multiply vectors and matrices, we need special rules. We can only multiply vectors of the same length, but we have to “transpose” one before we can do so. Consider two vectors of length <span class="math inline">\(p\)</span>, <span class="math inline">\(\mathbf{b}\)</span> and <span class="math inline">\(\mathbf{c}\)</span>.</p>
<p><span class="math display">\[
\mathbf{b}^T\mathbf{c} = \begin{bmatrix}
b_1 \\ b_2 \\ \vdots \\ b_p
\end{bmatrix}^T
\begin{bmatrix}
c_1 \\ c_2 \\ \vdots \\ c_p
\end{bmatrix} = 
\begin{bmatrix}
b_1 &amp; b_2 &amp; \cdots &amp; b_p
\end{bmatrix}
\begin{bmatrix}
c_1 \\ c_2 \\ \vdots \\ c_p
\end{bmatrix}
\]</span></p>
<p>The superscript <span class="math inline">\(T\)</span> in between the vectors stands for “transpose” (an apostrophe is sometimes used instead, as in <span class="math inline">\(\mathbf{b}&#39;\mathbf{c}\)</span>) and indicates that a vector or matrix should kind of tip over: all its columns become rows instead. That’s why <span class="math inline">\(\mathbf{b}\)</span> becomes a row vector (or <span class="math inline">\(1 \times p\)</span> matrix). The reason we do this is because we need dimensions to match up when we multiply vectors and matrices. That is because the number of elements in a row on the left (<span class="math inline">\(\mathbf{b}^T\)</span>) must equal the number of elements in a column on the right (<span class="math inline">\(\mathbf{c}\)</span>), so that we can multiply the corresponding elements and then sum over their products. Let’s look at <span class="math inline">\(\mathbf{b}^T\mathbf{c}\)</span>, where there is just one column-row pair to multiply and sum:</p>
<p><span class="math display">\[
\begin{bmatrix}
b_1 &amp; b_2 &amp; \cdots &amp; b_p
\end{bmatrix}
\begin{bmatrix}
c_1 \\ c_2 \\ \vdots \\ c_p
\end{bmatrix} = 
b_1c_1 + b_2c_2 + \cdots + b_pc_p
= \sum_{i = 1}^p b_ic_i
\]</span></p>
<p>We’ll also use the summation notation above a great deal. All three expressions tell us that we are summing over the products of each of the individual pairs of elements in the vectors <span class="math inline">\(\mathbf{b}\)</span> and <span class="math inline">\(\mathbf{c}\)</span>. It is clear why only two vectors of the same length can be multiplied. And think about what results from that multiplication: just a scalar (i.e., the product is of dimension <span class="math inline">\(1\times1\)</span>).</p>
<p>One situation that occurs often in statistics is the product of a vector with itself:</p>
<p><span class="math display">\[
\mathbf{b}^T\mathbf{b} = \sum_{i = 1}^p b_i^2
\]</span></p>
<p>Matrix notation makes it easy to write down a sum of squares.</p>
<p>When we multiply a vector by a matrix, the same idea applies, only this time we have more rows or columns to sum over.</p>
<p><span class="math display">\[
\mathbf{X}\mathbf{b} = 
\begin{bmatrix}
x_{11} &amp; x_{12} &amp; \cdots &amp; x_{1p} \\ 
x_{21} &amp; x_{22} &amp; \cdots &amp; x_{2p} \\ 
\vdots &amp;\vdots &amp; \ddots &amp; \vdots \\ 
x_{n1} &amp; x_{n2} &amp; \cdots &amp; x_{np}
\end{bmatrix} \begin{bmatrix}
b_1 \\ b_2 \\ \vdots \\ b_p
\end{bmatrix} = 
\begin{bmatrix}
x_{11}b_1 + x_{12}b_2 + \cdots x_{1p}b_p  \\
x_{21}b_1 + x_{22}b_2 + \cdots x_{2p}b_p  \\
\vdots \\
x_{n1}b_1 + x_{n2}b_2 + \cdots x_{np}b_p 
\end{bmatrix}
\]</span></p>
<p>If this is a new concept to you, study carefully what just happened. We multiplied the elements of the first row of <span class="math inline">\(\mathbf{X}\)</span> with those over the column vector <span class="math inline">\(\mathbf{b}\)</span> and summed: this became the first element of our new product (i.e., just like the scalar that results from the vector-vector multiplication above).Then we multiplied the second row of <span class="math inline">\(\mathbf{X}\)</span> with the column vector <span class="math inline">\(\mathbf{b}\)</span>; this became the second element of the new product, and so on. The product in its entirety is just a vector of length <span class="math inline">\(n\)</span>.</p>
<p>Let’s look at some other ways we could write this same product:</p>
<p><span class="math display">\[
\mathbf{X}\mathbf{b} = 
\begin{bmatrix}
\mathbf{x_1}^T\mathbf{b}\\
\mathbf{x_2}^T\mathbf{b}\\
\vdots \\
\mathbf{x_n}^T\mathbf{b}\\
\end{bmatrix} =
\begin{bmatrix}
\sum_{i = 1}^p x_{1i}b_i\\
\sum_{i = 1}^p x_{2i}b_i\\
\vdots \\
\sum_{i = 1}^p x_{ni}b_i\\
\end{bmatrix}
\]</span></p>
<p>The <span class="math inline">\(\mathbf{x_1}^T\mathbf{b}\)</span> form illustrates how each element is just a vector-vector product, where one vector is a row of <span class="math inline">\(\mathbf{X}\)</span> (so if we consider it as a column vector, must be transposed), which is lower-case and indexed by its row number to make clear that each is a different vector from that matrix. In each case, it should be clear that we’re using the same <span class="math inline">\(\mathbf{b}\)</span> vector in each new element of our product, but a different one of the <span class="math inline">\(n\)</span> rows of <span class="math inline">\(\mathbf{X}\)</span>.</p>
<p>Again the ideas extend to matrix-matrix multiplication. Let’s multiply the <span class="math inline">\(m\times n\)</span> matrix <span class="math inline">\(\mathbf{Y}\)</span> by <span class="math inline">\(\mathbf{X}\)</span>. The subscripts on the matrices below are used to clarify the dimensions (and provide an easy check that they are compatible). First we multiply the first row of <span class="math inline">\(\mathbf{Y}\)</span> and the first column of <span class="math inline">\(\mathbf{X}\)</span> – this is the <span class="math inline">\(1,1\)</span> element of our resulting matrix, which we’ll call <span class="math inline">\(\mathbf{Z}\)</span>. The appropriate rows are boldface just to show at each step the row and column that are multiplied and the placement in the resulting matrix.</p>
<p><span class="math display">\[
\mathbf{Y}_{m\times n}\mathbf{X}_{n\times p}
\]</span> <span class="math display">\[
= \begin{bmatrix}
\mathbf{y_{11}} &amp; \mathbf{y_{12}} &amp; \cdots &amp; \mathbf{y_{1n}} \\ 
y_{21} &amp; y_{22} &amp; \cdots &amp; y_{2n} \\ 
\vdots &amp;\vdots &amp; \ddots &amp; \vdots \\ 
y_{m1} &amp; y_{m2} &amp; \cdots &amp; y_{mn}
\end{bmatrix}
\begin{bmatrix}
\mathbf{x_{11}} &amp; x_{12} &amp; \cdots &amp; x_{1p} \\ 
\mathbf{x_{21}} &amp; x_{22} &amp; \cdots &amp; x_{2p} \\ 
\vdots &amp;\vdots &amp; \ddots &amp; \vdots \\ 
\mathbf{x_{n1}} &amp; x_{n2} &amp; \cdots &amp; x_{np}
\end{bmatrix} = 
\begin{bmatrix}
\mathbf{z_{11}} &amp;  &amp; \cdots &amp;  \\ 
 &amp;  &amp; \cdots &amp; \\ 
\vdots &amp;\vdots &amp; \ddots &amp; \vdots \\ 
 &amp;  &amp; \cdots &amp; 
\end{bmatrix}
\]</span></p>
<p><span class="math display">\[ = \begin{bmatrix}
\mathbf{y_{11}} &amp; \mathbf{y_{12}} &amp; \cdots &amp; \mathbf{y_{1n}} \\ 
y_{21} &amp; y_{22} &amp; \cdots &amp; y_{2n} \\ 
\vdots &amp;\vdots &amp; \ddots &amp; \vdots \\ 
y_{m1} &amp; y_{m2} &amp; \cdots &amp; y_{mn}
\end{bmatrix}
\begin{bmatrix}
x_{11} &amp; \mathbf{x_{12}} &amp; \cdots &amp; x_{1p} \\ 
x_{21} &amp; \mathbf{x_{22}} &amp; \cdots &amp; x_{2p} \\ 
\vdots &amp;\vdots &amp; \ddots &amp; \vdots \\ 
x_{n1} &amp; \mathbf{x_{n2}} &amp; \cdots &amp; x_{np}
\end{bmatrix} = 
\begin{bmatrix}
z_{11} &amp; \mathbf{z_{12}} &amp; \cdots &amp;  \\ 
 &amp;  &amp; \cdots &amp; \\ 
\vdots &amp;\vdots &amp; \ddots &amp; \vdots \\ 
 &amp;  &amp; \cdots &amp; 
\end{bmatrix}
\]</span></p>
<p><span class="math display">\[ = \begin{bmatrix}
y_{11} &amp; y_{12} &amp; \cdots &amp; y_{1n} \\ 
\mathbf{y_{21}} &amp; \mathbf{y_{22}} &amp; \cdots &amp; \mathbf{y_{2n}} \\
\vdots &amp;\vdots &amp; \ddots &amp; \vdots \\ 
y_{m1} &amp; y_{m2} &amp; \cdots &amp; y_{mn}
\end{bmatrix}
\begin{bmatrix}
\mathbf{x_{11}} &amp; x_{12} &amp; \cdots &amp; x_{1p} \\ 
\mathbf{x_{21}} &amp; x_{22} &amp; \cdots &amp; x_{2p} \\ 
\vdots &amp;\vdots &amp; \ddots &amp; \vdots \\ 
\mathbf{x_{n1}} &amp; x_{n2} &amp; \cdots &amp; x_{np}
\end{bmatrix} = 
\begin{bmatrix}
z_{11} &amp; z_{12} &amp; \cdots &amp;  \\ 
\mathbf{z_{21}} &amp;  &amp; \cdots &amp; \\ 
\vdots &amp;\vdots &amp; \ddots &amp; \vdots \\ 
 &amp;  &amp; \cdots &amp; 
\end{bmatrix}
\]</span></p>
<p>and so on until every row-column combination of <span class="math inline">\(\mathbf{Y}\)</span> and <span class="math inline">\(\mathbf{X}\)</span> respectively has been multiplied to form an element of <span class="math inline">\(\mathbf{Z}\)</span>. As before, each of those elements is the sum of the products of the corresponding elements in <span class="math inline">\(\mathbf{Y}\)</span> and <span class="math inline">\(\mathbf{X}\)</span>.</p>
<p>Think about each of those remaining steps necessary to fill in the rest of <span class="math inline">\(\mathbf{Z}\)</span>.</p>
<div class="panel panel-default">
<div data-label="dimZ" class="tutorial-question panel-body">
<div id="dimZ-answer_container" class="shiny-html-output"></div>
<div id="dimZ-message_container" class="shiny-html-output"></div>
<div id="dimZ-action_button_container" class="shiny-html-output"></div>
<script>if (Tutorial.triggerMathJax) Tutorial.triggerMathJax()</script>
</div>
</div>
<p>If you look back carefully at our examples, you’ll notice a pattern. First we multiplied a row vector by a column vector, or a <span class="math inline">\(1 \times p\)</span> matrix by a <span class="math inline">\(p \times 1\)</span> matrix, and ended up with a scalar of dimension <span class="math inline">\(1 \times 1\)</span>. Next we multiplied an <span class="math inline">\(n \times p\)</span> matrix by a <span class="math inline">\(p \times 1\)</span> vector and ended up with an <span class="math inline">\(n \times 1\)</span> vector. Finally, when we multiplied an <span class="math inline">\(m \times n\)</span> matrix by an <span class="math inline">\(n \times p\)</span> matrix, we got an <span class="math inline">\(m \times p\)</span> matrix.</p>
<p>In each case, we need the <em>inner</em> pair of dimensions to match, and the <em>outer</em> pair is the dimension of the resulting matrix.</p>
<p><span class="math display">\[\mathbf{Y}_{m\times n}\mathbf{X}_{n\times p} = \mathbf{Z}_{m \times p}\]</span></p>
<p>Like we saw with the two vectors above, sometimes we need to “transpose” one of the objects to make the dimensions compatible. With a vector, this just meant turning it on its side. With a matrix, the idea is the same: the first row becomes the first column, the second row the second column, and so on. Let’s look at a new matrix <span class="math inline">\(\mathbf{Q}\)</span>:</p>
<p><span class="math display">\[\mathbf{Q} = \begin{bmatrix}
r &amp; s &amp; t \\
u &amp; v &amp; w
\end{bmatrix}
\]</span></p>
<p><span class="math display">\[\mathbf{Q}^T = \begin{bmatrix}
r &amp; u \\
s &amp; v \\
t &amp; w
\end{bmatrix}
\]</span></p>
<p>If we wanted to multiply <span class="math inline">\(\mathbf{Q}\)</span> by itself (similar to squaring a number), first we need to transpose one of the matrices to make the dimension compatible:</p>
<p><span class="math display">\[\mathbf{Q}^T\mathbf{Q} = \begin{bmatrix}
r^2 + u^2 &amp; rs + uv &amp; rt + uw \\
rs + uv &amp; s^2 + v^2 &amp; st + vw \\
rt + uw &amp; st + vw &amp; t^2 + w^2
\end{bmatrix}
\]</span> Now try writing out <span class="math inline">\(\mathbf{Q}\mathbf{Q}^T\)</span>.</p>
<div class="panel panel-default">
<div data-label="dimQQt" class="tutorial-question panel-body">
<div id="dimQQt-answer_container" class="shiny-html-output"></div>
<div id="dimQQt-message_container" class="shiny-html-output"></div>
<div id="dimQQt-action_button_container" class="shiny-html-output"></div>
<script>if (Tutorial.triggerMathJax) Tutorial.triggerMathJax()</script>
</div>
</div>
<p>What do you notice about <span class="math inline">\(\mathbf{Q}^T\mathbf{Q}\)</span> and <span class="math inline">\(\mathbf{Q}\mathbf{Q}^T\)</span>? They are special types of matrices called <strong>symmetric</strong> matrices. This means that the element in the <span class="math inline">\(1,2\)</span> position is the same as that in the <span class="math inline">\(2,1\)</span> position, the element in the <span class="math inline">\(1,3\)</span> position is the same as that in the <span class="math inline">\(3,1\)</span> position, and so on. The diagonal (which in these two cases consists of sums of squares) can be anything. Formally, a matrix is symmetric if it is equal to its own transpose, i.e., if <span class="math inline">\(\mathbf{X} = \mathbf{X}^T\)</span>. Importantly, correlation and covariance matrices are always symmetric.</p>
<p>There is a special symmetric matrix called the <strong>identity</strong> matrix. What this means is that we can multiply any matrix by the identity and get the same matrix we started with. When we’re working with scalars only, the number 1 acts just like the identity: <span class="math inline">\(a \times 1 = a\)</span>. When we’re working with matrices, we use <span class="math inline">\(\mathbf{I}\)</span> to indicate the identity matrix: <span class="math inline">\(\mathbf{XI} = \mathbf{X}\)</span>. We define the identity as the matrix with 1’s on the diagonal and 0’s on the off-diagonals, with whatever dimensions are necessary to be compatible with <span class="math inline">\(\mathbf{X}\)</span>:</p>
<p><span class="math display">\[\mathbf{I} = 
\begin{bmatrix}
1 &amp; 0 &amp; 0 &amp; \cdots \\
0 &amp; 1 &amp; 0 &amp; \cdots \\
0 &amp; 0 &amp; 1  &amp;\cdots\\
\vdots &amp; \vdots &amp; \vdots &amp; \ddots
\end{bmatrix}
\]</span></p>
<p>The identity matrix has a special property not generally true of matrices: <span class="math inline">\(\mathbf{IX} = \mathbf{XI}\)</span>, meaning that it doesn’t matter in which order you do the matrix multiplication. Except in special cases, this is not otherwise true: <span class="math inline">\(\mathbf{XY} \neq \mathbf{YX}\)</span>.</p>
<p>There’s one more important type of matrix you should know about. Like other functions, matrices can often be inverted. An inverse of a function is kind of like the function that takes you back to your starting point. For scalars, <span class="math inline">\(a^{-1}=\frac{1}{a}\)</span> is the multiplicative inverse of <span class="math inline">\(a\)</span>: when we multiply the two together, we get 1. Similarly, for a square matrix <span class="math inline">\(\mathbf{X}\)</span>, there sometimes exists an <strong>inverse matrix</strong> <span class="math inline">\(\mathbf{X}^{-1}\)</span> such that <span class="math inline">\(\mathbf{X}^{-1}\mathbf{X} = \mathbf{I}\)</span> and <span class="math inline">\(\mathbf{X}\mathbf{X}^{-1} = \mathbf{I}\)</span>.</p>
<p>The process of finding <span class="math inline">\(\mathbf{X}^{-1}\)</span> is called inverting a matrix, but it’s not always possible. In particular, if the row vectors or column vectors that make up <span class="math inline">\(\mathbf{X}\)</span> are not <strong>linearly independent</strong>, then the inverse <span class="math inline">\(\mathbf{X}^{-1}\)</span> doesn’t exist. Linear independence occurs when none of the vectors can be written as a linear sum of the other vectors – each holds unique information. An intuitive example of linear <em>dependence</em> is if you have data on a sample consisting of vectors of their ages at the time of survey, years of birth, and years surveyed. Since year surveyed = year of birth + age at survey, one of those vectors contains redundant information (it doesn’t matter which one), and they are not linearly independent.</p>
<p>Remember above when we saw that <span class="math inline">\(a\mathbf{X}\)</span> just meant multiplying each element in <span class="math inline">\(\mathbf{X}\)</span> by the scalar <span class="math inline">\(a\)</span>? Well that means that we can factor values out of a matrix. This may give some intuition for the following rule: <span class="math inline">\((a\mathbf{X})^{-1} = \frac{1}{a}\mathbf{X}^{-1}\)</span>. We can in some sense invert <span class="math inline">\(a\)</span> and <span class="math inline">\(\mathbf{X}\)</span> separately, because <span class="math inline">\(a\)</span> is a scalar and can be factored out. We still have that <span class="math inline">\((\frac{1}{a}\mathbf{X}^{-1})(a\mathbf{X}) = \mathbf{I}\)</span>.</p>
<div id="section-matrices-and-their-multiplication-in-r" class="section level3">
<h3>Matrices and their multiplication in R</h3>
<div class="tutorial-exercise-support" data-label="mat-prep" data-caption="Code" data-completion="1" data-diagnostics="1" data-startover="1" data-lines="0">
<pre class="text"><code>set.seed(6789)
vals &lt;- rnorm(6)
mat &lt;- matrix(vals, nrow = 2, ncol = 3)
vec1 &lt;- 1:3
vec2 &lt;- 4:6
other_mat &lt;- matrix(rnorm(12), nrow = 3, ncol = 4)
set.seed(6789)</code></pre>
</div>
<p>We’ve seen vectors in R; now let’s look at matrices, which are another type of object. Like vectors, but unlike dataframes, all the elements of a matrix in R must be of the same class. So we could have character matrices or logical matrices, but we’ll just look at numerical matrices for now. To make a matrix we use the function <code>matrix()</code>, with the first argument a vector of the values we want to fill the matrix. These values get filled in column-by-column unless we specify <code>byrow = TRUE</code>. We tell R the dimensions of the matrix we want with the arguments <code>nrow =</code> and <code>ncol =</code>.</p>
Compare the two matrices composed of random numbers from a standard normal distribution.
<div class="tutorial-exercise" data-label="mat" data-caption="Code" data-completion="1" data-diagnostics="1" data-startover="1" data-lines="3">
<pre class="text"><code>vals &lt;- rnorm(6)
matrix(vals, nrow = 2, ncol = 3)
matrix(vals, nrow = 2, ncol = 3, byrow = TRUE)</code></pre>
<script type="application/json" data-opts-chunk="1">{"fig.width":6.5,"fig.height":4,"fig.retina":2,"fig.align":"center","fig.keep":"high","fig.show":"asis","out.width":624,"warning":true,"error":false,"message":true,"exercise.df_print":"paged","exercise.timelimit":60,"exercise.setup":"mat-prep","exercise.checker":"NULL"}</script>
</div>
<p>Notice how the rows and columns are labeled in square brackets. For the rows, the indexing value precedes a comma, for the columns, the index follows the comma. We can extract row or column vectors or single elements with the same approach. Try replacing the values in the square brackets to extract different pieces of this matrix.</p>
<div class="tutorial-exercise" data-label="matExt" data-caption="Code" data-completion="1" data-diagnostics="1" data-startover="1" data-lines="4">
<pre class="text"><code>mat
mat[2,] # second row
mat[,3] # third column
mat[2,3] # 2,3 element</code></pre>
<script type="application/json" data-opts-chunk="1">{"fig.width":6.5,"fig.height":4,"fig.retina":2,"fig.align":"center","fig.keep":"high","fig.show":"asis","out.width":624,"warning":true,"error":false,"message":true,"exercise.df_print":"paged","exercise.timelimit":60,"exercise.setup":"mat-prep","exercise.checker":"NULL"}</script>
</div>
What happens if you use an index that is larger than the matrix itself?
<div class="tutorial-exercise" data-label="matErr" data-caption="Code" data-completion="1" data-diagnostics="1" data-startover="1" data-lines="1">
<pre class="text"><code>mat[4,4]</code></pre>
<script type="application/json" data-opts-chunk="1">{"fig.width":6.5,"fig.height":4,"fig.retina":2,"fig.align":"center","fig.keep":"high","fig.show":"asis","out.width":624,"warning":true,"error":true,"message":true,"exercise.df_print":"paged","exercise.timelimit":60,"exercise.setup":"mat-prep","exercise.checker":"NULL"}</script>
</div>
Before we multiply anything, let’s try out the transpose function <code>t()</code>, because we’re going to need it. It does just what you’d expect.
<div class="tutorial-exercise" data-label="transpose" data-caption="Code" data-completion="1" data-diagnostics="1" data-startover="1" data-lines="1">
<pre class="text"><code>t(mat)</code></pre>
<script type="application/json" data-opts-chunk="1">{"fig.width":6.5,"fig.height":4,"fig.retina":2,"fig.align":"center","fig.keep":"high","fig.show":"asis","out.width":624,"warning":true,"error":false,"message":true,"exercise.df_print":"paged","exercise.timelimit":60,"exercise.setup":"mat-prep","exercise.checker":"NULL"}</script>
</div>
You have to be careful doing vector and matrix multiplication in R because it requires special functions. If we just use the regular multiplication command <code>*</code>, we perform element-wise multiplication. Here we start with two vectors of length 3 and end up with another vector of length 3, whereas if we had performed true matrix multiplication we would have ended up with a scalar.
<div class="tutorial-exercise" data-label="badMult" data-caption="Code" data-completion="1" data-diagnostics="1" data-startover="1" data-lines="0">
<pre class="text"><code>vec1 &lt;- 1:3
vec2 &lt;- 4:6
vec1 * vec2</code></pre>
<script type="application/json" data-opts-chunk="1">{"fig.width":6.5,"fig.height":4,"fig.retina":2,"fig.align":"center","fig.keep":"high","fig.show":"asis","out.width":624,"warning":true,"error":false,"message":true,"exercise.df_print":"paged","exercise.timelimit":60,"exercise.setup":"mat-prep","exercise.checker":"NULL"}</script>
</div>
<p>Before running the following code, calculate the value we should get when multiplying the vectors <code>1, 2, 3</code> and <code>4, 5, 6</code> together. Then, to do true matrix multiplication, use the <code>%*%</code> function.</p>
<div class="tutorial-exercise" data-label="goodMult" data-caption="Code" data-completion="1" data-diagnostics="1" data-startover="1" data-lines="1">
<pre class="text"><code>t(vec1) %*% vec2</code></pre>
<script type="application/json" data-opts-chunk="1">{"fig.width":6.5,"fig.height":4,"fig.retina":2,"fig.align":"center","fig.keep":"high","fig.show":"asis","out.width":624,"warning":true,"error":false,"message":true,"exercise.df_print":"paged","exercise.timelimit":60,"exercise.setup":"mat-prep","exercise.checker":"NULL"}</script>
</div>
<p>Let’s confirm our result with a very tedious way of multiplying the two vectors: by extracting the elements that need to be multiplied one at a time, and summing.</p>
<div class="tutorial-exercise" data-label="longMult" data-caption="Code" data-completion="1" data-diagnostics="1" data-startover="1" data-lines="1">
<pre class="text"><code>vec1[1] * vec2[1] + vec1[2] * vec2[2] + vec1[3] * vec2[3]</code></pre>
<script type="application/json" data-opts-chunk="1">{"fig.width":6.5,"fig.height":4,"fig.retina":2,"fig.align":"center","fig.keep":"high","fig.show":"asis","out.width":624,"warning":true,"error":false,"message":true,"exercise.df_print":"paged","exercise.timelimit":60,"exercise.setup":"mat-prep","exercise.checker":"NULL"}</script>
</div>
<p>Notice how when we extract an element of a vector, we don’t need a comma, because there is only one dimension we could be pulling from.</p>
<p>Does that match with what you expected?</p>
<p>Now let’s try to predict how to multiply a vector and a matrix in R. Recalling that that <code>vec1</code> is a column vector of length 3 and <code>mat</code> is a <span class="math inline">\(2 \times 3\)</span> matrix, try to answer the following question.</p>
<div class="panel panel-default">
<div data-label="matVecMult" class="tutorial-question panel-body">
<div id="matVecMult-answer_container" class="shiny-html-output"></div>
<div id="matVecMult-message_container" class="shiny-html-output"></div>
<div id="matVecMult-action_button_container" class="shiny-html-output"></div>
<script>if (Tutorial.triggerMathJax) Tutorial.triggerMathJax()</script>
</div>
</div>
When we try to multiply objects with dimensions that aren’t compatible, R gives us a warning.
<div class="tutorial-exercise" data-label="matMultErr" data-caption="Code" data-completion="1" data-diagnostics="1" data-startover="1" data-lines="1">
<pre class="text"><code>vec1 %*% mat</code></pre>
<script type="application/json" data-opts-chunk="1">{"fig.width":6.5,"fig.height":4,"fig.retina":2,"fig.align":"center","fig.keep":"high","fig.show":"asis","out.width":624,"warning":true,"error":true,"message":true,"exercise.df_print":"paged","exercise.timelimit":60,"exercise.setup":"mat-prep","exercise.checker":"NULL"}</script>
</div>
<p>Although it can be tempting to just try the <code>t()</code> function until something works, make sure you are calculating the product you expect and that it has the dimensions you expect. Use the box below to compare <code>t(vec1) %*% vec2</code> and <code>vec1 %*% t(vec2)</code>. Before you run the code, predict what you will see.</p>
<div class="tutorial-exercise" data-label="products" data-caption="Code" data-completion="1" data-diagnostics="1" data-startover="1" data-lines="2">
<script type="application/json" data-opts-chunk="1">{"fig.width":6.5,"fig.height":4,"fig.retina":2,"fig.align":"center","fig.keep":"high","fig.show":"asis","out.width":624,"warning":true,"error":false,"message":true,"exercise.df_print":"paged","exercise.timelimit":60,"exercise.setup":"mat-prep","exercise.checker":"NULL"}</script>
</div>
<p>Now let’s multiply two matrices. Besides the <span class="math inline">\(2 \times 3\)</span> matrix <code>mat</code>, you also have access to a <span class="math inline">\(3 \times 4\)</span> matrix <code>other_mat</code>.</p>
<div class="panel panel-default">
<div data-label="dimRq" class="tutorial-question panel-body">
<div id="dimRq-answer_container" class="shiny-html-output"></div>
<div id="dimRq-message_container" class="shiny-html-output"></div>
<div id="dimRq-action_button_container" class="shiny-html-output"></div>
<script>if (Tutorial.triggerMathJax) Tutorial.triggerMathJax()</script>
</div>
</div>
<p>Multiply the two matrices <code>mat</code> and <code>mat_prod</code> and store their product in the object <code>mat_prod</code>. Then use the <code>class()</code> and <code>dim()</code> functions on that object to see if you ended up with a matrix with the right dimensions.</p>
<div class="tutorial-exercise" data-label="dimProd" data-caption="Code" data-completion="1" data-diagnostics="1" data-startover="1" data-lines="3">
<script type="application/json" data-opts-chunk="1">{"fig.width":6.5,"fig.height":4,"fig.retina":2,"fig.align":"center","fig.keep":"high","fig.show":"asis","out.width":624,"warning":true,"error":false,"message":true,"exercise.df_print":"paged","exercise.timelimit":60,"exercise.setup":"mat-prep","exercise.checker":"NULL"}</script>
</div>
<div class="tutorial-exercise-support" data-label="dimProd-solution" data-caption="Code" data-completion="1" data-diagnostics="1" data-startover="1" data-lines="0">
<pre class="text"><code>mat_prod &lt;- mat %*% other_mat
class(mat_prod)
dim(mat_prod)</code></pre>
</div>
<p>One matrix that is very easy to create in R is the identity matrix <span class="math inline">\(\mathbf{I}\)</span>. The function <code>diag()</code> has several uses. First of all, if you give it a square matrix (e.g., <code>diag(square_mat)</code>), it will pull off the diagonal elements as a vector. Secondly, it will create an identity matrix if instead of a matrix you give it a single number (e.g., <code>diag(6)</code>). The value will give it will be the dimension of the resulting matrix (recall that the identity matrix is always square).</p>
Explore what happens if you use the <code>diag()</code> function on a matrix that isn’t square.
<div class="tutorial-exercise" data-label="nonsquare" data-caption="Code" data-completion="1" data-diagnostics="1" data-startover="1" data-lines="2">
<pre class="text"><code>mat
diag(mat)</code></pre>
<script type="application/json" data-opts-chunk="1">{"fig.width":6.5,"fig.height":4,"fig.retina":2,"fig.align":"center","fig.keep":"high","fig.show":"asis","out.width":624,"warning":true,"error":false,"message":true,"exercise.df_print":"paged","exercise.timelimit":60,"exercise.setup":"mat-prep","exercise.checker":"NULL"}</script>
</div>
Then look at what the identity matrix looks like:
<div class="tutorial-exercise" data-label="idPlain" data-caption="Code" data-completion="1" data-diagnostics="1" data-startover="1" data-lines="1">
<pre class="text"><code>diag(6)</code></pre>
<script type="application/json" data-opts-chunk="1">{"fig.width":6.5,"fig.height":4,"fig.retina":2,"fig.align":"center","fig.keep":"high","fig.show":"asis","out.width":624,"warning":true,"error":false,"message":true,"exercise.df_print":"paged","exercise.timelimit":60,"exercise.checker":"NULL"}</script>
</div>
<p>Now create an identity matrix with which you can pre-multiply (as in <span class="math inline">\(\mathbf{IX}\)</span>) <code>mat</code> and another with which you can post-multiply (as in <span class="math inline">\(\mathbf{XI}\)</span>) <code>mat</code>. Store them as <code>id1</code> and <code>id2</code> and confirm that when you perform the appropriate multiplication, you end up with the original <code>mat</code> matrix.</p>
<div class="tutorial-exercise" data-label="ids" data-caption="Code" data-completion="1" data-diagnostics="1" data-startover="1" data-lines="5">
<script type="application/json" data-opts-chunk="1">{"fig.width":6.5,"fig.height":4,"fig.retina":2,"fig.align":"center","fig.keep":"high","fig.show":"asis","out.width":624,"warning":true,"error":false,"message":true,"exercise.df_print":"paged","exercise.timelimit":60,"exercise.setup":"mat-prep","exercise.checker":"NULL"}</script>
</div>
<div class="tutorial-exercise-support" data-label="ids-solution" data-caption="Code" data-completion="1" data-diagnostics="1" data-startover="1" data-lines="0">
<pre class="text"><code>id1 &lt;- diag(2)
id2 &lt;- diag(3)
mat
id1 %*% mat
mat %*% id2</code></pre>
</div>
<p>Finally, let’s add in one more helpful function when working with matrices in R. The <code>solve()</code> function will find the inverse of a matrix, if it exists. First let’s look at the error message we get when the inverse <em>doesn’t</em> exist. We’ll make a <span class="math inline">\(3 \times 3\)</span> matrix with columns made from the two vectors we already made, along with a third column that is just 2 times the first. This makes it so the columns of the matrix are <strong>linearly dependent</strong>, and it can’t be inverted.</p>
<div class="tutorial-exercise" data-label="singular" data-caption="Code" data-completion="1" data-diagnostics="1" data-startover="1" data-lines="3">
<pre class="text"><code>mat3 &lt;- cbind(vec1, vec2, 2*vec1)
mat3
solve(mat3)</code></pre>
<script type="application/json" data-opts-chunk="1">{"fig.width":6.5,"fig.height":4,"fig.retina":2,"fig.align":"center","fig.keep":"high","fig.show":"asis","out.width":624,"warning":true,"error":true,"message":true,"exercise.df_print":"paged","exercise.timelimit":60,"exercise.setup":"mat-prep","exercise.checker":"NULL"}</script>
</div>
<p>Now let’s put it all together. One expression that pops up a lot in statistics is <span class="math inline">\((\mathbf{X}^T\mathbf{X})^{-1}\)</span>. Create a <span class="math inline">\(3 \times 2\)</span> matrix <code>X</code> using random values from a standard normal distribution. Convert <span class="math inline">\((\mathbf{X}^T\mathbf{X})^{-1}\)</span> into R code and store the object you create using whatever name you’d like. Then use other functions that you know to explore the object.</p>
<div class="tutorial-exercise" data-label="matFin" data-caption="Code" data-completion="1" data-diagnostics="1" data-startover="1" data-lines="6">
<script type="application/json" data-opts-chunk="1">{"fig.width":6.5,"fig.height":4,"fig.retina":2,"fig.align":"center","fig.keep":"high","fig.show":"asis","out.width":624,"warning":true,"error":false,"message":true,"exercise.df_print":"paged","exercise.timelimit":60,"exercise.setup":"mat_prep","exercise.checker":"NULL"}</script>
</div>
<div class="tutorial-exercise-support" data-label="matFin-solution" data-caption="Code" data-completion="1" data-diagnostics="1" data-startover="1" data-lines="0">
<pre class="text"><code>X &lt;- matrix(rnorm(6), ncol = 2, nrow = 3)
obj &lt;- solve(t(X) %*% X)
class(obj)
dim(obj)
summary(obj)</code></pre>
</div>
<div id="section-more-resources-2" class="section level4">
<h4>More resources</h4>
<p><a href="https://www.khanacademy.org/math/precalculus/precalc-matrices">This</a> whole section has a lot of great information and practice with matrices. For a more advanced introduction, work through the sections on vectors, linear combinations, and linear dependence <a href="https://www.khanacademy.org/math/linear-algebra/vectors-and-spaces">here</a>. You can also pick and choose from the videos <a href="https://www.khanacademy.org/math/linear-algebra/matrix-transformations">here</a>, particularly those on functions and linear transformations.</p>
</div>
</div>
</div>
<div id="section-models-and-estimation" class="section level2">
<h2>Models and estimation</h2>
<div id="section-estimation-of-population-parameters" class="section level4">
<h4>Estimation of population parameters</h4>
<p>When we have a quantitative research question in the population health sciences, we usually are interested estimating in population <strong>parameters</strong>, or values that define something about a population. Maybe this is <span class="math inline">\(\mu\)</span>, the mean blood pressure in a population, or <span class="math inline">\(\boldsymbol \beta\)</span>, a vector of logistic regression coefficients for risk factors for esophageal cancer. We’ll use <span class="math inline">\(\theta\)</span> in this section to represent the parameter or vector of parameters that we are interested in estimating.</p>
<p>These parameters are values that are true for a given population – maybe the probability of disease, or an exposure-disease risk ratio. Since we don’t have access to the entire population, we rely on a sample to <strong>estimate</strong> these parameters. In this tutorial, we’ll assume throughout that we have a <em>random</em> sample. We’ll relax that assumption later in the course.</p>
<p>We express the fact that the actual values we get are estimates by putting “hats” on the same notation we used for the parameters, such as <span class="math inline">\(\hat{\theta}\)</span>. You can read this “theta-hat”.</p>
<p>Sometimes these parameters describe a model. A model is essentially a structure that we assume represents the distribution of the data we have collected. The key word here is <em>assume</em> – modeling generally involves making choices about this structure in order to best approximate reality, while knowing that we will never be able to mimic it perfectly. While assumptions may be inevitable, it is important to be able to recognize and acknowledge those that we make – and those that others make – and, when appropriate, evaluate the sensitivity of these assumptions to other modeling choices we could have made instead.</p>
<p>Other times we can avoid making particular types of assumptions and estimate parameters <strong>non-parametrically</strong>. For example, if we want to estimate the mean blood pressure in a population, we don’t need to say anything about the distribution of blood pressure – it doesn’t have to be normal, for example – in order to calculate a sample mean.</p>
<p>The sample mean is one example of an <strong>estimator</strong>. This is a function of the data: a tool or algorithm that takes in observations and produces an estimate for a given parameter. The sample mean <span class="math inline">\(\frac{1}{n} \sum_{i = 1}^n X_i\)</span> is an estimator of the population mean. (The sample mean gets the special notation <span class="math inline">\(\bar{X}\)</span>, read “x-bar”.) The sample variance <span class="math inline">\(\frac{1}{n - 1} \sum_{i = 1}^n (X_i - \bar{X})^2\)</span> is an estimator of the population variance. The ordinary least squares estimator is used to estimate regression coefficients for linear regressions.</p>
</div>
<div id="section-unbiasedness" class="section level4">
<h4>Unbiasedness</h4>
<p>Note that the sample variance as expressed above has in the denominator <span class="math inline">\(n - 1\)</span>. This may seem weird, and indeed <span class="math inline">\(\frac{1}{n} \sum_{i = 1}^n (X_i - \bar{X})^2\)</span> is another estimator of the population variance. In many cases there are multiple plausible estimators for the same quantity. In fact, you could choose anything as an estimator – say, take the first observation’s value <span class="math inline">\(X_1\)</span> as an estimator of the population mean <span class="math inline">\(E[X]\)</span>. Or just take the fixed constant 0.5 as an estimator of the population risk. Clearly these are really bad ideas!</p>
<p>We choose estimators based on certain qualities. One desirable quality is <strong>unbiasedness</strong>. For an estimator to be unbiased, it must on average give us the “right answer”. Formally, we can express this as <span class="math display">\[E[\hat{\theta}] = \theta\]</span> This may seem weird. Previously we’ve been taking expectations of random variables. Well, <span class="math inline">\(\hat{\theta}\)</span> <em>is</em> a random variable. Any function of random variables is itself a random variable. Let’s use some data to compare the two estimators of the variance and see how each random variable behaves.</p>
<p>For simplicity, we’ll assume that we are sampling a random variable <span class="math inline">\(X\)</span> that is distributed Normal(0,1). That distributional assumption is not necessary to use these estimators, but we want to start with something we <em>know</em> the right answer to. That is, we know that the true variance of <span class="math inline">\(X\)</span> is 1. We’ll call that parameter of interest <span class="math inline">\(\sigma^2\)</span>. We’ll compare these two estimators of <span class="math inline">\(\sigma^2\)</span>: <span class="math display">\[\hat{\sigma}_A^2 = \frac{1}{n - 1} \sum_{i = 1}^n (X_i - \bar{X})^2\]</span> <span class="math display">\[\hat{\sigma}_B^2 = \frac{1}{n} \sum_{i = 1}^n (X_i - \bar{X})^2\]</span> We have no idea what the distributions of <span class="math inline">\(\hat{\sigma}_A^2\)</span> and <span class="math inline">\(\hat{\sigma}_B^2\)</span> are – although we could figure it out mathematically, it’s a lot easier to find via simulation.</p>
<p>A simulation allows us to repeat experiments over and over. Each of the dice-rolling and ball-drawing examples we looked at earlier was examined via simulation. In the real world, we could draw one sample of <span class="math inline">\(X\)</span> (let’s say <span class="math inline">\(n = 20\)</span>) and estimate both <span class="math inline">\(\hat{\sigma}_A^2\)</span> and <span class="math inline">\(\hat{\sigma}_B^2\)</span> from that single sample:</p>
<div class="tutorial-exercise-support" data-label="unbias-prep" data-caption="Code" data-completion="1" data-diagnostics="1" data-startover="1" data-lines="0">
<pre class="text"><code>set.seed(6789)
n &lt;- 20
X &lt;- rnorm(20) # draw 20 values from standard normal
A &lt;- sum((X - mean(X))^2) / (n - 1)
B &lt;- sum((X - mean(X))^2) / n
set.seed(123) # set seed for reproducible results
res &lt;- matrix(NA, nrow = 1000, ncol = 2) # empty matrix for results
for (j in 1:1000){
  X &lt;- rnorm(20) # sample from standard normal
  A &lt;- sum((X - mean(X))^2) / (n - 1) # use estimator A
  B &lt;- sum((X - mean(X))^2) / n # use estimator B
  res[j, ] &lt;- c(A, B) # store in next row of dataset
}</code></pre>
</div>
<div class="tutorial-exercise" data-label="a" data-caption="Code" data-completion="1" data-diagnostics="1" data-startover="1" data-lines="7">
<pre class="text"><code>set.seed(6789) # set seed for reproducible results
n &lt;- 20 # sample size
X &lt;- rnorm(20) # draw 20 values from standard normal
A &lt;- sum((X - mean(X))^2) / (n - 1) # estimator A
B &lt;- sum((X - mean(X))^2) / n # estimator B
A
B</code></pre>
<script type="application/json" data-opts-chunk="1">{"fig.width":6.5,"fig.height":4,"fig.retina":2,"fig.align":"center","fig.keep":"high","fig.show":"asis","out.width":624,"warning":true,"error":false,"message":true,"exercise.df_print":"paged","exercise.timelimit":60,"exercise.setup":"unbias-prep","exercise.checker":"NULL"}</script>
</div>
<p>So we have estimates of <span class="math inline">\(\sigma^2\)</span>: <span class="math inline">\(\hat{\sigma}_A^2 = 1.280\)</span> and <span class="math inline">\(\hat{\sigma}_B^2 = 1.216\)</span>. But since we only have one sample we have no way to compare the two estimators. Which one is closer to the truth? Well, we know that the true <span class="math inline">\(\sigma^2 = 1\)</span>, so <span class="math inline">\(\hat{\sigma}_B^2\)</span> looks better, but that could just be a fluke, since the values of <span class="math inline">\(X\)</span> are random. Besides, we don’t know the truth in real life. What we need to do is find the expectation of each estimator by simulating its distribution – sampling over and over – and taking the mean.</p>
<p>One way to repeat something a number of times in R is through the use of a for-loop. Here’s a simple for-loop. The first line chooses in indexing variable – here, <code>i</code> – and tells R what values to iterate through. On the first iteration, <code>i = 1</code>, the second <code>i = 2</code>, and so on. In between the curly brackets, the code is evaluated for each value of <code>i</code>.</p>
<div class="tutorial-exercise" data-label="for1" data-caption="Code" data-completion="1" data-diagnostics="1" data-startover="1" data-lines="4">
<pre class="text"><code>for (i in 1:5){
  z &lt;- i + 2
  print(z)
}</code></pre>
<script type="application/json" data-opts-chunk="1">{"fig.width":6.5,"fig.height":4,"fig.retina":2,"fig.align":"center","fig.keep":"high","fig.show":"asis","out.width":624,"warning":true,"error":false,"message":true,"exercise.df_print":"paged","exercise.timelimit":60,"exercise.checker":"NULL"}</script>
</div>
<p>Try it yourself, based on the code above. Iterate through the values 5, 10, and 15 with the variable <code>q</code>. Print out the values of <code>q</code> divided by 5.</p>
<div class="tutorial-exercise" data-label="for2" data-caption="Code" data-completion="1" data-diagnostics="1" data-startover="1" data-lines="5">
<script type="application/json" data-opts-chunk="1">{"fig.width":6.5,"fig.height":4,"fig.retina":2,"fig.align":"center","fig.keep":"high","fig.show":"asis","out.width":624,"warning":true,"error":false,"message":true,"exercise.df_print":"paged","exercise.timelimit":60,"exercise.checker":"NULL"}</script>
</div>
<div class="tutorial-exercise-support" data-label="for2-solution" data-caption="Code" data-completion="1" data-diagnostics="1" data-startover="1" data-lines="0">
<pre class="text"><code>for(q in c(5, 10, 15)){
  print(q/5)
}</code></pre>
</div>
<p>There are many possible solutions and only one is shown here. You may have noticed that in a for-loop a line of code that just has a variable name won’t print out unless you explicitly use the <code>print()</code> function.</p>
<p>In our simulation, however, we don’t want to just print out the values of <span class="math inline">\(\hat{\sigma}_A^2\)</span> and <span class="math inline">\(\hat{\sigma}_B^2\)</span>; we want to store them somewhere. We’ll do that by creating an empty matrix (filling it with <code>NA</code> values, which are R’s way of dealing with missing data). Since we are going to iterate 1000 times and we have 2 values we want to save, the dimensions of the matrix will be 1000 x 2.</p>
<div class="tutorial-exercise" data-label="sim1" data-caption="Code" data-completion="1" data-diagnostics="1" data-startover="1" data-lines="8">
<pre class="text"><code>set.seed(123) # set seed for reproducible results
res &lt;- matrix(NA, nrow = 1000, ncol = 2) # empty matrix for results
for (j in 1:1000){
  X &lt;- rnorm(20) # sample from standard normal
  A &lt;- sum((X - mean(X))^2) / (n - 1) # use estimator A
  B &lt;- sum((X - mean(X))^2) / n # use estimator B
  res[j, ] &lt;- c(A, B) # store in next row of dataset
}</code></pre>
<script type="application/json" data-opts-chunk="1">{"fig.width":6.5,"fig.height":4,"fig.retina":2,"fig.align":"center","fig.keep":"high","fig.show":"asis","out.width":624,"warning":true,"error":false,"message":true,"exercise.df_print":"paged","exercise.timelimit":60,"exercise.checker":"NULL"}</script>
</div>
<p>The last line of the loop takes the values of the estimators and replaces the <code>j</code>th row of the results matrix with them (remember that leaving the value after the comma blank in the square matrix means you’re referencing the entire row). So after one iteration, only the first row will hold values, and the rest will still be missing; after the second iteration both of the first two rows will hold values, etc.</p>
Use some of the functions you know to explore the <code>res</code> object below. One that may be helpful is <code>dim()</code>, which tells you the dimensions of an object.
<div class="tutorial-exercise" data-label="sim2" data-caption="Code" data-completion="1" data-diagnostics="1" data-startover="1" data-lines="4">
<script type="application/json" data-opts-chunk="1">{"fig.width":6.5,"fig.height":4,"fig.retina":2,"fig.align":"center","fig.keep":"high","fig.show":"asis","out.width":624,"warning":true,"error":false,"message":true,"exercise.df_print":"paged","exercise.timelimit":60,"exercise.setup":"unbias-prep","exercise.checker":"NULL"}</script>
</div>
<p>If you tried to use <code>mean()</code>on the res object, you may have noticed that it combined the columns of the matrix and took the overall mean. That’s not what we want – we want the mean of each column separately. What we are trying to estimate is <span class="math inline">\(E[\hat{\sigma}^2_A]\)</span> from the mean of the first column and <span class="math inline">\(E[\hat{\sigma}^2_B]\)</span> from the mean of the second column.</p>
<p>One solution is to extract the columns separately and take the mean of each one with <code>mean(res[,1])</code> and <code>mean(res[,2])</code>. However, an easier solution is <code>colMeans(res)</code> (there’s also <code>rowMeans()</code>), which will do both at once.</p>
<div class="tutorial-exercise" data-label="sim3" data-caption="Code" data-completion="1" data-diagnostics="1" data-startover="1" data-lines="1">
<pre class="text"><code>colMeans(res)</code></pre>
<script type="application/json" data-opts-chunk="1">{"fig.width":6.5,"fig.height":4,"fig.retina":2,"fig.align":"center","fig.keep":"high","fig.show":"asis","out.width":624,"warning":true,"error":false,"message":true,"exercise.df_print":"paged","exercise.timelimit":60,"exercise.setup":"unbias-prep","exercise.checker":"NULL"}</script>
</div>
<p>Remember that to be unbiased, we want <span class="math inline">\(E[\hat{\sigma}^2] = \sigma^2\)</span>. We know that <span class="math inline">\(\sigma^2 = 1\)</span> because we drew <span class="math inline">\(X\)</span> from a distribution with variance 1 (the standard normal). We can see from this simulation that <span class="math inline">\(\hat{\sigma}^2_A\)</span> is an unbiased estimator, while <span class="math inline">\(\hat{\sigma}^2_B\)</span> underestimates the true value of <span class="math inline">\(\sigma^2\)</span>. (Note that we’ll never get exactly 1 because the simulation has its own random error, but we got close enough to be satisfied.)</p>
<p>This conflicts with what we thought at first: remember that <span class="math inline">\(\hat{\sigma}^2_B\)</span> from our first sample was closer to 1! We don’t know anything about the expected value of an estimator just from one sample.</p>
</div>
<div id="section-consistency" class="section level4">
<h4>Consistency</h4>
<p>Another desirable characteristic for an estimator is <strong>consistency</strong>. This is less mathematically obvious, so we won’t go into much detail, but in words it basically means that as your sample size gets bigger, the estimate you get from that sample gets closer and closer to the true value of the parameter.</p>
<p>The difference between unbiasedness and consistency may seem subtle. But note that unbiasedness doesn’t depend on sample size (there’s no <span class="math inline">\(n\)</span> in the definition), whereas consistency does. The estimator that just uses data from the first observation in the dataset <span class="math inline">\(X_1\)</span> is unbiased: on average, as you sample from the population over and over again, the first person’s value will be the population value (assuming your observations are in random order). However, no matter how big your sample gets, you’re still only using data from one person, so your estimate from a single sample is never going to be any closer to the true parameter than if you had only sampled one person.</p>
</div>
<div id="section-efficiency" class="section level4">
<h4>Efficiency</h4>
<p>Consistency is a pretty straightforward requirement for an estimator. But we may be OK with a little bit of bias if the estimator is particularly <strong>efficient</strong>, or has low variance. Unlike consistency and unbiasedness, efficiency (or precision) is not an absolute concept – it’s not that an estimator is or is not efficient, only that it is or is not relative to another estimator. If we compare two estimators, <span class="math inline">\(\hat{\theta}\)</span> and <span class="math inline">\(\tilde{\theta}\)</span>, the relative efficiency of <span class="math inline">\(\hat{\theta}\)</span> means that <span class="math inline">\(Var(\hat{\theta}) &lt; Var(\tilde{\theta})\)</span>, or that on average, you’ll get an estimate that’s closer to the true value of the parameter when you use <span class="math inline">\(\hat{\theta}\)</span>.</p>
<p>Let’s return to our example where we take 20 samples from a standard normal distribution. Instead of estimating <span class="math inline">\(\sigma^2\)</span>, now let’s estimate the mean, <span class="math inline">\(\mu\)</span>. Again, we chose this distribution because we know the right answer: <span class="math inline">\(\mu = 0\)</span>. But now when we compare two estimators we care about whether the estimates are similar to each other – whether they have low variability.</p>
<p>The two estimators of <span class="math inline">\(\mu\)</span> we’ll compare are the sample mean and the sample median. Since the normal distribution is symmetric, its mean is equal to its median, so they are both estimators of the same quantity. We want to know which one has lower variance.</p>
<p>We can reuse most of the code from last time. Fill in the missing lines to reflect the two estimators we are now comparing, where <span class="math inline">\(\hat{\mu}_A\)</span> is the sample mean and <span class="math inline">\(\hat{\mu}_B\)</span> the sample median.</p>
<div class="tutorial-exercise" data-label="medSim" data-caption="Code" data-completion="1" data-diagnostics="1" data-startover="1" data-lines="8">
<pre class="text"><code>set.seed(123) # set seed for reproducible results
res &lt;- matrix(NA, nrow = 1000, ncol = 2) # empty matrix for results
for (j in 1:1000){
  X &lt;- rnorm(20) # sample from standard normal
  A &lt;-  
  B &lt;- 
  res[j, ] &lt;- c(A, B) # store in next row of dataset
}</code></pre>
<script type="application/json" data-opts-chunk="1">{"fig.width":6.5,"fig.height":4,"fig.retina":2,"fig.align":"center","fig.keep":"high","fig.show":"asis","out.width":624,"warning":true,"error":true,"message":true,"exercise.df_print":"paged","exercise.timelimit":60,"exercise.checker":"NULL"}</script>
</div>
<p>First check to confirm that both are unbiased estimators of <span class="math inline">\(\mu\)</span>, which in this case is 0:</p>
<div class="tutorial-exercise" data-label="medBias" data-caption="Code" data-completion="1" data-diagnostics="1" data-startover="1" data-lines="1">
<script type="application/json" data-opts-chunk="1">{"fig.width":6.5,"fig.height":4,"fig.retina":2,"fig.align":"center","fig.keep":"high","fig.show":"asis","out.width":624,"warning":true,"error":false,"message":true,"exercise.df_print":"paged","exercise.timelimit":60,"exercise.checker":"NULL"}</script>
</div>
<p>What do you think?</p>
<p>Before we compare the efficiency of these estimators, let’s look at their <strong>sampling distributions</strong> as a whole. Here are histograms made from each of the estimates. (The code is essentially <code>hist[,1]</code> and <code>hist[,2]</code>, but a bit more complicated than that so it’s not shown. Feel free to check out the help file for <code>hist()</code> and play around.) <img src="index_files/figure-html/index-5-1.png" width="624" style="display: block; margin: auto;" /></p>
<p>This is the empirical equivalent to the probability density function. Recall that <span class="math inline">\(\hat{\mu}_A\)</span> and <span class="math inline">\(\hat{\mu}_B\)</span> are just continuous random variables. We haven’t said anything about <em>which</em> distribution each has, but we can describe its characteristics and calculate values like its mean and variance. If it helps to convince you that these sampling distributions are just like the probability distributions we’ve seen, we can turn those histograms into “density plots”, which basically just smooth over the histogram bars. <img src="index_files/figure-html/index-6-1.png" width="624" style="display: block; margin: auto;" /></p>
<p>The distribution that has smaller variance tells us which of these estimators is more efficient.</p>
<div class="panel panel-default">
<div data-label="efficient" class="tutorial-question panel-body">
<div id="efficient-answer_container" class="shiny-html-output"></div>
<div id="efficient-message_container" class="shiny-html-output"></div>
<div id="efficient-action_button_container" class="shiny-html-output"></div>
<script>if (Tutorial.triggerMathJax) Tutorial.triggerMathJax()</script>
</div>
</div>
<p>We can confirm that by calculating the variance of each of the estimates. Unfortunately there’s no direct equivalent to <code>colMeans()</code> for the variance in base R. Use the <code>var()</code> function on each of the columns separately to compare.</p>
<div class="tutorial-exercise" data-label="effCalc" data-caption="Code" data-completion="1" data-diagnostics="1" data-startover="1" data-lines="2">
<script type="application/json" data-opts-chunk="1">{"fig.width":6.5,"fig.height":4,"fig.retina":2,"fig.align":"center","fig.keep":"high","fig.show":"asis","out.width":624,"warning":true,"error":false,"message":true,"exercise.df_print":"paged","exercise.timelimit":60,"exercise.checker":"NULL"}</script>
</div>
<div class="tutorial-exercise-support" data-label="effCalc-solution" data-caption="Code" data-completion="1" data-diagnostics="1" data-startover="1" data-lines="0">
<pre class="text"><code>var(res[,1])
var(res[,2])</code></pre>
</div>
<p>Again, we would never have known this from just one sample. With our one sample, we may happen to get values of <span class="math inline">\(X\)</span> that give us estimates near the center, or way out in the tails, and we will have no idea which. We want an efficient estimator so the tails are more compact: if we do happen to get a “bad” sample way out there, it won’t be quite as far out!</p>
</div>
<div id="section-more-resources-3" class="section level4">
<h4>More resources</h4>
<p>Learn more about sampling distributions and estimators <a href="https://www.khanacademy.org/math/ap-statistics/sampling-distribution-a">here</a>.</p>
</div>
</div>
<div id="section-statistical-inference" class="section level2">
<h2>Statistical inference</h2>
<div id="section-standard-errors" class="section level4">
<h4>Standard errors</h4>
<p>When we do “statistics”, we are using data from a sample to tell us something about the population. In the language of the last section, we are using observations to estimate parameters. OK, great. We use our estimator – and hopefully we chose a good one – and calculate an estimate of the prevalence of depression, or the odds ratio for smoking and lung cancer is, or whatever it is we had a question about. But we only used a single sample to get that estimate! We also want to know, in general, how certain we are that that value is a good measure of what’s going on in the population. If we had chosen a completely different random sample, how different would we expect our estimate to be?</p>
<p>We saw in the last section that our estimates have a distribution. But we only got to see the sampling distribution when we ran simulations. What can we learn about that distribution from only a single sample?</p>
<p>Well, we can assess the variability of our estimates with <strong>standard errors</strong>. Smaller standard errors mean that we can be more certain that an estimate is close to the true population value of whatever parameter it is we’re trying to estimate. A standard error is simply the standard deviation of the sampling distribution. We calculated the variance of the sampling distribution of two estimators of <span class="math inline">\(\mu\)</span> in the last section – we could simply take their square roots to get the standard deviation of those distributions, or the standard errors.</p>
<p>Before we see how to estimate these from one sample, take a second to think about what leads to small standard errors. We already compared two estimators in the same samples. What about when we compare the same estimator in different samples?</p>
<div class="panel panel-default">
<div data-label="ses" class="tutorial-question panel-body">
<div id="ses-answer_container" class="shiny-html-output"></div>
<div id="ses-message_container" class="shiny-html-output"></div>
<div id="ses-action_button_container" class="shiny-html-output"></div>
<script>if (Tutorial.triggerMathJax) Tutorial.triggerMathJax()</script>
</div>
</div>
<p>Since we’ve already compared estimators, let’s just stick with one: the sample mean. Intuitively, if we’re trying to estimate the mean height in a population and we sample 1000 people, we’ll get a more precise estimate than if we only sample 100 people. That is, we’re more likely to be closer to the truth with a larger sample size. If we’re trying to estimate mean height among adults and mean height among 2-year-olds, we’ll get a more precise estimate for the toddlers. That’s because the random variable of interest, height, has a smaller variance in 2-year-olds (they tend to be more similar in size). Our single sample is less likely to have extreme values, so the sample mean is more likely to be close to the population mean.</p>
<p>Before we move on, let’s practice designing simulations to demonstrate both of these concepts. First, let’s compare the two sample sizes, 100 and 1000. A number of the lines in the following code are incomplete, but the structure is almost identical to the previous for loops we ran. Now instead of comparing two estimators, we are using the same estimator in two samples. Fill in the missing pieces.</p>
<div class="tutorial-exercise" data-label="nComp" data-caption="Code" data-completion="1" data-diagnostics="1" data-startover="1" data-lines="13">
<pre class="text"><code># set seed for reproducible results
n1 &lt;-  # first sample size
n2 &lt;-  # second sample size to compare
res &lt;-  # empty matrix for results
for (j in 1:1000){
  X1 &lt;- # first sample from standard normal
  X2 &lt;- # second sample from standard normal
  mean1 &lt;- # estimate from first sample
  mean2 &lt;- # estimate from second sample
  res[j, ] &lt;- c(mean1, mean2) # store in next row of dataset
}
sd(res[,1])
sd(res[,2]) # standard error = sd of sampling distribution</code></pre>
<script type="application/json" data-opts-chunk="1">{"fig.width":6.5,"fig.height":4,"fig.retina":2,"fig.align":"center","fig.keep":"high","fig.show":"asis","out.width":624,"warning":true,"error":true,"message":true,"exercise.df_print":"paged","exercise.timelimit":60,"exercise.checker":"NULL"}</script>
</div>
<div class="tutorial-exercise-support" data-label="nComp-solution" data-caption="Code" data-completion="1" data-diagnostics="1" data-startover="1" data-lines="0">
<pre class="text"><code>set.seed(6789) # set seed for reproducible results
n1 &lt;- 100 # first sample size
n2 &lt;- 1000 # second sample size to compare
res &lt;- matrix(NA, ncol = 2, nrow = 1000) # empty matrix for results
for (j in 1:1000){
  X1 &lt;- rnorm(n1) # first sample from standard normal
  X2 &lt;- rnorm(n2) # second sample from standard normal
  mean1 &lt;- mean(X1) # estimate from first sample
  mean2 &lt;- mean(X2) # estimate from second sample
  res[j, ] &lt;- c(mean1, mean2) # store in next row of dataset
}
sd(res[,1])
sd(res[,2]) # standard error = sd of sampling distribution</code></pre>
</div>
<p>What are your conclusions? Hopefully you found that a larger sample size leads to a smaller standard error! If you want, use the <code>hist()</code> function to make a picture of the sampling distribution so you can confirm that the one with the larger sample size is tighter.</p>
<p>Now let’s try estimating the means of two different random variables, each with a different variance. Let’s say the first is again standard normal, but the second is <span class="math inline">\(Normal(0, 2^2)\)</span> (remember that the argument for <code>rnorm()</code> is <code>sd =</code>, which is why I wrote the parameter like that). This time you’re on your own (but feel free to copy and paste code from earlier!). You can use whatever sample size you’d like.</p>
<div class="tutorial-exercise" data-label="varComp" data-caption="Code" data-completion="1" data-diagnostics="1" data-startover="1" data-lines="13">
<script type="application/json" data-opts-chunk="1">{"fig.width":6.5,"fig.height":4,"fig.retina":2,"fig.align":"center","fig.keep":"high","fig.show":"asis","out.width":624,"warning":true,"error":false,"message":true,"exercise.df_print":"paged","exercise.timelimit":60,"exercise.checker":"NULL"}</script>
</div>
<div class="tutorial-exercise-support" data-label="varComp-solution" data-caption="Code" data-completion="1" data-diagnostics="1" data-startover="1" data-lines="0">
<pre class="text"><code>set.seed(456) # set seed for reproducible results
res &lt;- matrix(NA, ncol = 2, nrow = 1000) # empty matrix for results
for (j in 1:1000){
  X1 &lt;- rnorm(100) # first sample from standard normal
  X2 &lt;- rnorm(100, sd = 2) # second sample from normal(0, 4)
  mean1 &lt;- mean(X1) # estimate from first sample
  mean2 &lt;- mean(X2) # estimate from second sample
  res[j, ] &lt;- c(mean1, mean2) # store in next row of dataset
}
sd(res[,1])
sd(res[,2]) # standard error = sd of sampling distribution</code></pre>
</div>
<p>What are your conclusions now? The standard errors clearly depend on the variability of the random variable that we’re sampling.</p>
</div>
<div id="section-the-central-limit-theorem" class="section level4">
<h4>The central limit theorem</h4>
<p>These two ideas come together in the central limit theorem, which provides a way to approximate sampling distributions from just a single, independent sample. We won’t go into the mathematics behind it, but essentially it tells us that for statistics that are based on sums of independent random variables (like the sample mean), their sampling distributions are asymptotically normal with a variance that depends on the sample size and the variance of the random variable being summed.</p>
<p>For the sample mean <span class="math inline">\(\bar{X}\)</span>, this means that as the sample size grows to infinity, estimates over repeated samples will tend to be normally distributed with a mean that’s the expectation of <span class="math inline">\(X\)</span> and variance that’s the variance of <span class="math inline">\(X\)</span> divided by the sample size.</p>
<p><span class="math display">\[ \bar{X} \stackrel{n\rightarrow \infty}{\sim} Normal( E[X], \frac{Var(X)}{n})\]</span> Of course, we never have sample sizes even close to infinity, so we generally just say “in large enough samples”, the distribution is “approximately” normal. Since the standard error is the square root of the variance of the sampling distribution, the standard error of the sample mean is simply <span class="math inline">\(\sqrt{Var(X)}/\sqrt{n}\)</span>. Since we often refer to the variance of a random variable as <span class="math inline">\(\sigma^2\)</span>, we often denote this as <span class="math inline">\(\frac{\sigma}{\sqrt{n}}\)</span>.</p>
<div class="panel panel-default">
<div data-label="TF1" class="tutorial-question panel-body">
<div id="TF1-answer_container" class="shiny-html-output"></div>
<div id="TF1-message_container" class="shiny-html-output"></div>
<div id="TF1-action_button_container" class="shiny-html-output"></div>
<script>if (Tutorial.triggerMathJax) Tutorial.triggerMathJax()</script>
</div>
</div>
<div class="panel panel-default">
<div data-label="TF2" class="tutorial-question panel-body">
<div id="TF2-answer_container" class="shiny-html-output"></div>
<div id="TF2-message_container" class="shiny-html-output"></div>
<div id="TF2-action_button_container" class="shiny-html-output"></div>
<script>if (Tutorial.triggerMathJax) Tutorial.triggerMathJax()</script>
</div>
</div>
<p>Note that we haven’t said <em>anything</em> about the distribution of <span class="math inline">\(X\)</span> itself: the central limit theorem refers to the distribution of <span class="math inline">\(\bar{X}\)</span>, the sampling distribution. In fact, <span class="math inline">\(X\)</span> could be anything: any of the distributions we’ve seen so far, or any other distribution (OK, there are some limitations, but none you’ll ever come across in real life). When you think about it, that’s pretty crazy. You can take a sample mean of just about anything, and the distribution of that sample mean will be normal with known mean and variance. That means we don’t actually need to take repeated samples to know the (approximate) sampling distribution! And since we know the sampling distribution, we can estimate standard errors.</p>
<div class="panel panel-default">
<div data-label="cltPois" class="tutorial-question panel-body">
<div id="cltPois-answer_container" class="shiny-html-output"></div>
<div id="cltPois-message_container" class="shiny-html-output"></div>
<div id="cltPois-action_button_container" class="shiny-html-output"></div>
<script>if (Tutorial.triggerMathJax) Tutorial.triggerMathJax()</script>
</div>
</div>
<p>Now let’s design a simulation to confirm that the central limit theorem holds and the answer from the question above was truly the correct (approximate) standard error. I’ll tell you what to do in English; you convert it into code. Make sure your results are reproducible.</p>
<ol style="list-style-type: lower-alpha">
<li>Create an object to hold your results</li>
<li>Sample from the Poisson(9) distribution with <span class="math inline">\(n = 100\)</span></li>
<li>Take the sample mean</li>
<li>Repeat steps b-c a large number of times</li>
<li>Calculate a standard error from your estimates</li>
</ol>
<p>(You may also want to create a histogram to see that they look normal with mean 9)</p>
<div class="tutorial-exercise" data-label="PoisSim" data-caption="Code" data-completion="1" data-diagnostics="1" data-startover="1" data-lines="12">
<script type="application/json" data-opts-chunk="1">{"fig.width":6.5,"fig.height":4,"fig.retina":2,"fig.align":"center","fig.keep":"high","fig.show":"asis","out.width":624,"warning":true,"error":false,"message":true,"exercise.df_print":"paged","exercise.timelimit":60,"exercise.checker":"NULL"}</script>
</div>
<div class="tutorial-exercise-support" data-label="PoisSim-solution" data-caption="Code" data-completion="1" data-diagnostics="1" data-startover="1" data-lines="0">
<pre class="text"><code>set.seed(987)
res &lt;- rep(NA, 1000)
for(i in 1:1000) {
  X &lt;- rpois(100, 9)
  res[i] &lt;- mean(X)
}
sd(res)
hist(res)</code></pre>
</div>
<p>It turns out that a lot of the estimators we use are based on means and therefore follow the central limit theorem. We can always estimate the variance of the random variable from our sample, and of course we always know the sample size, so the central limit theorem shows up <em>everywhere</em>.</p>
</div>
<div id="section-p-values" class="section level4">
<h4>P-values</h4>
<p>Many researchers in the population health sciences and other fields have a love-hate relationship with p-values. They are easy to calculate, show up everywhere, and do have some meaning if interpreted correctly. However, they are often misinterpreted, emphasized too heavily, and used to justify conclusions that aren’t warranted. That said, you should be one of the few who understand them, so let’s dive in.</p>
<p>When answering a research question, we usually have a hypothesis in mind – perhaps that some exposure and outcome are related. In the population, this would mean that the exposed group has a different risk of disease than the unexposed group. Importantly, that hypothesized relationship implies a <strong>null hypothesis</strong> as well: there is <em>no</em> difference between the groups, or no difference from a specified value. If we denote the difference between the groups with <span class="math inline">\(\theta\)</span>, then we can write the null and alternative hypotheses, respectively: <span class="math display">\[H_0: \theta = 0\]</span> <span class="math display">\[H_A: \theta \neq 0\]</span> Note that here we’re referring to the population parameter <span class="math inline">\(\theta\)</span>, not some estimate <span class="math inline">\(\hat{\theta}\)</span>. We only get to see <span class="math inline">\(\hat{\theta}\)</span> from our sample, but our question is inherently about the population we care about. In our given sample, there will surely be a difference in whatever it is we’re measuring; it might be very small or it might be big. Whether that difference exists in the population is what we care about. Of course, it might not, and it may be that we only see a difference in our sample due to sampling variability.</p>
<p>The goal of the p-value is to quantify how likely it is that we would see that difference in our sample if there were truly no difference in the population. That is, what is the probability that we would estimate the <span class="math inline">\(\hat{\theta}\)</span> we did if <span class="math inline">\(\theta = 0\)</span>?</p>
<p>Let’s assume we’re using a sample size and an unbiased estimator to which the central limit theorem applies. Then we know that <span class="math inline">\(\hat{\theta} \sim Normal(\theta, \frac{\sigma^2}{n})\)</span>. If <span class="math inline">\(\theta = 0\)</span>, as under the null hypothesis, then <span class="math inline">\(\hat{\theta} \sim Normal(0, \frac{\sigma^2}{n})\)</span>. For notational simplicity, let’s refer to <span class="math inline">\(\frac{\sigma^2}{n}\)</span> as <span class="math inline">\(se^2\)</span>, for standard error squared.</p>
<p>Now let’s <strong>standardize</strong> our estimate. This means that we divide it by its standard deviation (which is of course the standard error, since we’re talking about a sampling distribution): <span class="math inline">\(\frac{\hat{\theta}}{se}\)</span>. What this allows us to do is count standard deviation by 1’s instead of by multiples of <span class="math inline">\(se\)</span>. So the value <span class="math inline">\(se\)</span> used to be 1 standard deviation away from the mean, now the value 1 itself is 1 standard deviation away from the mean. In other words, <em>under the null hypothesis</em>, <span class="math display">\[\frac{\hat{\theta}}{se}\sim Normal(0, 1)\]</span></p>
<p><img src="index_files/figure-html/normPic-1.png" width="624" style="display: block; margin: auto;" /></p>
<p>To reiterate, the pdf above is centered at 0 because we are working under the null hypothesis. Calculating a p-value requires assuming that the null hypothesis is true; in this case, that <span class="math inline">\(\theta = 0\)</span>.</p>
<p>Now, we know what value of <span class="math inline">\(\hat{\theta}\)</span> we actually got from our sample. We know the value of <span class="math inline">\(se\)</span> because we’ve been able to calculate it using the central limit theorem. To make this more concrete, let’s take the value <span class="math inline">\(se = 2.5\)</span>, and imagine that <span class="math inline">\(\hat{\theta} = 5\)</span>.</p>
<p>Together those values give us <span class="math inline">\(\frac{\hat{\theta}}{se} = 2\)</span>. That means that the <span class="math inline">\(\hat{\theta}\)</span> value we estimated was 2 standard deviations from 0. Let’s look at it on the standard normal pdf.</p>
<p><img src="index_files/figure-html/normPic2-1.png" width="624" style="display: block; margin: auto;" /></p>
<p>Just eyeballing it, we can say that this is a pretty unlikely value in this distribution. In other words, it’s a pretty unlikely value to have been estimated from a population in which the null hypothesis is true. But just how unlikely? That’s what the p-value tells us.</p>
<p>Think back to when we first learned about continuous distributions and pdfs and cdfs. We saw that when we looked at a pdf, we could calculate probabilities as the area under the curve. Specifically, we could integrate over part of the pdf to find the probability of a value in that interval. We don’t care about the probability of the actual value of <span class="math inline">\(\hat{\theta}\)</span> that we got (remember that with a continuous distribution, that probability is 0 anyway). Instead, we care about a value as or more extreme – that is, equally or further away from 0.</p>
<p>The picture below highlights the probability we’re talking about: <img src="index_files/figure-html/normPic3-1.png" width="624" style="display: block; margin: auto;" /></p>
<p>This picture shows in blue the total probability of drawing a value from a standard normal distribution that is less than -2 or greater than 2. If <span class="math inline">\(\theta = 0\)</span>, this is the total probability of drawing a sample for which <span class="math inline">\(\frac{\hat{\theta}}{se}\leq -2\)</span> or <span class="math inline">\(\frac{\hat{\theta}}{se} &gt; 2\)</span>. What are the chances that we’d get something this crazy just due to bad luck?</p>
Hopefully you remember how to calculate this area! For the left-hand side of the distribution, we need to calculate <span class="math inline">\(P(\frac{\hat{\theta}}{se} \leq 2)\)</span> for <span class="math inline">\(\frac{\hat{\theta}}{se} \sim Normal(0, 1)\)</span>. That’s just the cdf of a standard normal: <span class="math inline">\(F(-2)\)</span>. And the other side is just its mirror image, so all we have to do is double that area. See if you can calculate this value in R:
<div class="tutorial-exercise" data-label="pCalc" data-caption="Code" data-completion="1" data-diagnostics="1" data-startover="1" data-lines="1">
<script type="application/json" data-opts-chunk="1">{"fig.width":6.5,"fig.height":4,"fig.retina":2,"fig.align":"center","fig.keep":"high","fig.show":"asis","out.width":624,"warning":true,"error":false,"message":true,"exercise.df_print":"paged","exercise.timelimit":60,"exercise.checker":"NULL"}</script>
</div>
<div class="tutorial-exercise-support" data-label="pCalc-solution" data-caption="Code" data-completion="1" data-diagnostics="1" data-startover="1" data-lines="0">
<pre class="text"><code>2 * pnorm(-2)</code></pre>
</div>
<p>You should get a p-value of 0.0455. Can you put into words what this value means?</p>
<p>There’s a lot more to statistical inference (and all of these topics!) than what’s on this page. For example, we didn’t cover confidence intervals, which are usually a better idea than p-values. Luckily you have a whole year of PHS 2000 – and the rest of your careers – to put this into practice and learn more!</p>
</div>
<div id="section-final-challenge" class="section level4">
<h4>Final challenge</h4>
<p>Write code to calculate a p-value for a difference of two means without using the central limit theorem or any explicit probability calculations (no <code>pnorm()</code>, etc.). Everything else is up to you.</p>
<p>Hint: Generate your observed data once. Then think about what it means for the null hypothesis to be true.</p>
<p>If you need more hints or want to check your thinking or your code, email <a href="mailto:louisa_h_smith@g.harvard.edu">louisa_h_smith@g.harvard.edu</a>.</p>
</div>
</div>
<div id="section-glossary" class="section level2">
<h2>Glossary</h2>
<p><strong>Bernoulli</strong> The distribution of a random variable that takes on values 1 and 0 with probabilities <span class="math inline">\(p\)</span> and <span class="math inline">\(1 − p\)</span>, respectively; <span class="math inline">\(f(x) = p^x(1 − p)^{1−x}\)</span>.</p>
<p><strong>binomial</strong> The distribution of the number of 1s in a series of <span class="math inline">\(n\)</span> independent 0,1 trials, each with probability <span class="math inline">\(p\)</span>; <span class="math inline">\(f(x) = \binom{n}{x} p^x(1 − p)^{n−x}\)</span>.</p>
<p><strong>central limit theorem</strong> (CLT) States, loosely, that the mean <span class="math inline">\(x\)</span> from a sample of size <span class="math inline">\(n\)</span> converges to a normal distribution with a mean equal to the population mean <span class="math inline">\(E[X]\)</span> and variance <span class="math inline">\(\frac{Var(X)}{n}\)</span> as <span class="math inline">\(n\)</span> approaches <span class="math inline">\(\infty\)</span>.</p>
<p><strong>confidence interval</strong> A range of plausible parameter values around a statistic, constructed so that it will contain the true value in a fixed proportion (usually 95%) of repeated samples.</p>
<p><strong>continuous</strong> Can take on any real-numbered value in a range.</p>
<p><strong>correlation</strong> Measures the strength and direction of the linear relationship between two variables; <span class="math inline">\(\rho\)</span>.</p>
<p><strong>covariance</strong> Measures the direction and extent of the linear relationship between two variables; <span class="math inline">\(Cov(X, Y)\)</span>.</p>
<p><strong>cumulative distribution function</strong> (cdf) Gives the probability that a random variable <span class="math inline">\(X\)</span> takes on value less than or equal to <span class="math inline">\(x\)</span>; <span class="math inline">\(F(x) = P(X \leq x)\)</span>.</p>
<p><strong>discrete</strong> Takes on a countable number of possible values.</p>
<p><strong>estimator</strong> A function used to estimate a parameter using observed data.</p>
<p><strong>expectation</strong> The probability-weighted average over all possible values in a distribution, also called expected value; <span class="math inline">\(E[X]\)</span> or <span class="math inline">\(\mu\)</span>.</p>
<p><strong>independent</strong> Refers to two variables whose distributions do not depend on the values of the other; <span class="math inline">\(X Y \implies Cov(X,Y)=0\)</span>.</p>
<p><strong>independent and identically distributed</strong> (i.i.d.) Random variables that have the same probability distribution and are independent.</p>
<p><strong>law of large numbers</strong> (LLN) Tells us that the average, <span class="math inline">\(\bar{X}\)</span>, over more and more repeated trials will converge to the expected value, <span class="math inline">\(E[X]\)</span>.</p>
<p><strong>normal</strong> A common continuous distribution, also called Gaussian; <span class="math inline">\(f(x) = \frac{1}{\sqrt{2\pi\sigma^2}}\exp\left(-\frac{1}{2}\left(\frac{x - \mu}{\sigma}\right)^2\right)\)</span></p>
<p><strong>null hypothesis</strong> The hypothesis we attempt to reject with the data, often stating that there is no relationship between two variables; <span class="math inline">\(H_0\)</span>.</p>
<p><strong>parameter</strong> A (possibly unknown) characteristic of a probability distribution.</p>
<p><strong>Poisson</strong> A discrete distribution that takes on integer values from 0 to <span class="math inline">\(\infty\)</span>, determined by the rate parameter <span class="math inline">\(\lambda\)</span>; <span class="math inline">\(f(x) = \frac{\lambda^x e^{-\lambda}}{x!}\)</span>.</p>
<p><strong>probability density function</strong> (pdf) The function of a continuous random variable that defines the relative likelihood of possible values, or the absolute probability of a range of values.</p>
<p><strong>probability distribution</strong> A description of the possible values that a random variable can take on and their likelihoods.</p>
<p><strong>probability mass function</strong> (pmf) Gives the probability that a discrete random variable is equal to each of its possible values; <span class="math inline">\(P(X = x)\)</span>.</p>
<p><strong>random variable</strong> The unknown numerical outcome of some random event.</p>
<p><strong>sampling distribution</strong> The probability distribution of a statistic calculated from a random sample.</p>
<p><strong>standard deviation</strong> Measures the variability of a random variable in the same units as that variable; <span class="math inline">\(\sqrt{Var(X)}\)</span> or <span class="math inline">\(\sigma\)</span>.</p>
<p><strong>standard error</strong> The standard deviation of the sampling distribution of a statistic; for the sample mean, <span class="math inline">\(\frac{\sigma}{\sqrt{n}}\)</span>.</p>
<p><strong>standard normal</strong> Refers to the normal distribution with mean 0 and variance 1.</p>
<p><strong>standardize</strong> Transform into units of mean 0 and standard deviation 1 by subtracting the mean and dividing by the standard deviation.</p>
<p><strong>statistic</strong> A function of a set of observations.</p>
<p><strong>test statistic</strong> A statistic with a cutoff that determines whether or not a null hypothesis is to be rejected.</p>
<p><strong>type I error</strong> The probability of rejecting the null hypothesis, given that the null hypothesis is true.</p>
<p><strong>variance</strong> Measures how far values of a variable generally are from the mean; <span class="math inline">\(Var(X)\)</span> or <span class="math inline">\(\sigma^2\)</span>.</p>
</div>
<div id="section-more-r-resources" class="section level2">
<h2>More R resources</h2>
<p>Used to SAS or STATA? These guides ( <a href="https://github.com/asnr/sas-to-r/blob/master/README.md">SAS</a> and Stata ( <a href="https://github.com/EconometricsBySimulation/RStata/wiki/Dictionary:-Stata-to-R">1</a> &amp; <a href="http://dss.princeton.edu/training/RStata.pdf">2</a>)) can help you translate from those languages to R.</p>
<p>Some people love learning R with <a href="https://swirlstats.com">Swirl</a>, which teaches you to code interactively.</p>
<p><a href="https://www.rstudio.com/online-learning/">Here’s</a> RStudio’s great list of online resources. In particular, there are some more online tutorials <a href="https://rstudio.cloud/learn/primers">here</a>.</p>
<p><a href="http://hadley.nz">Hadley Wickham</a> is probably the #1 R guru and has written several books about R, which you can read on his website, where you can also learn more about the packages he’s written, including <code>ggplot2</code>.</p>
<p><a href="http://www.moderndive.com">Here’s</a> another book with a good introduction to data science R, including data visualization.</p>
<p>The <code>fivethirtyeight</code> package has a ton of cool <a href="https://cloud.r-project.org/web/packages/fivethirtyeight/vignettes/fivethirtyeight.html">datasets</a> that you can play around with.</p>
<p>If you’re not understanding an error message, clear your workspace and/or restart RStudio and try again. Does the error still show up? Then try writing a <a href="https://en.wikipedia.org/wiki/Minimal_Working_Example">minimal working example</a>. What does it take to reproduce the error? Is the problem with your data, your code, or both?</p>
<p>Watch <a href="https://www.rstudio.com/resources/videos/debugging-techniques-in-rstudio/">this video</a> of an expert walk through her process of debugging code (even experts get error messages all the time!).</p>
<p>It may sound silly, but copying and pasting error messages into Google is usually the fastest way to solve a tricky problem. You will almost certainly end up on the relevant <a href="https://stackoverflow.com/">stack overflow</a> question page, because someone somewhere has experienced the error you’ve encountered.</p>
<p>Struggling with ggplot? Take a look at this <a href="https://www.rstudio.com/wp-content/uploads/2015/03/ggplot2-cheatsheet.pdf">cheat sheet</a> or this <a href="http://www.r-graph-gallery.com/all-graphs/">gallery</a>. <a href="http://www.cookbook-r.com/Graphs/">This website</a> is another great resource.</p>
<p>Ready to make your plots beautiful? Choose your color scheme with the RColorBrewer package. Explore ColorBrewer palettes <a href="http://colorbrewer2.org">here</a>. “Set1” and “Dark2” are favorites for qualitative data and “BuGn” is nice for sequential gradients.</p>
<p>This <a href="https://www.rstudio.com/wp-content/uploads/2015/03/rmarkdown-reference.pdf">Rmarkdown cheat sheet</a> is helpful for getting started.</p>
<p>Can’t remember the name of a certain Greek letter? Try <a href="http://detexify.kirelabs.org/classify.html">detexify</a>.</p>

<script type="application/shiny-prerendered" data-context="server-start">
library(learnr)
knitr::opts_chunk$set(fig.align = "center",
                      cache = TRUE)
tutorial_options(exercise.timelimit = 60)
options(digits = 5, scipen = 999)

pmf_func <- function(x, fx){
  par(mgp = c(3, 1, 0), mar = c(4, 4, 1, 1))
  plot(x, fx, type = "h", col = "blue", xaxt = "n", main = "pmf", ylab = "", xlab = "")
  abline(h = 0)
  points(x, fx, col = "blue", pch = 19)
  axis(1, xaxp = c(min(x), max(x), length(x) - 1))
  title(ylab="f(x)", line = 2, xlab = "x")
}

cdf_func <- function(x, fx){
  par(mgp = c(3, 1, 0), mar = c(4, 4, 1, 1))
  cdf <- c(0, cumsum(fx))
  plot(stepfun(x, cdf),xlab = "",ylab = "", verticals = FALSE, do.points = TRUE, pch = 16, main = "cdf", col = "red")
  points(x, cdf[-length(cdf)], col = "red")
  title(ylab="F(x)", line = 2, xlab = "x")
}

</script>
 
<script type="application/shiny-prerendered" data-context="server">
learnr:::register_http_handlers(session, metadata = NULL)
</script>
 
<script type="application/shiny-prerendered" data-context="server">
session$onSessionEnded(function() {
        learnr:::session_stop_event(session)
      })
</script>
 
<script type="application/shiny-prerendered" data-context="server">
`tutorial-exercise-a-result` <- learnr:::setup_exercise_handler(reactive(req(input$`tutorial-exercise-a-code-editor`)), session)
output$`tutorial-exercise-a-output` <- renderUI({
  `tutorial-exercise-a-result`()
})
</script>
 
<script type="application/shiny-prerendered" data-context="server">
`tutorial-exercise-for1-result` <- learnr:::setup_exercise_handler(reactive(req(input$`tutorial-exercise-for1-code-editor`)), session)
output$`tutorial-exercise-for1-output` <- renderUI({
  `tutorial-exercise-for1-result`()
})
</script>
 
<script type="application/shiny-prerendered" data-context="server">
`tutorial-exercise-for2-result` <- learnr:::setup_exercise_handler(reactive(req(input$`tutorial-exercise-for2-code-editor`)), session)
output$`tutorial-exercise-for2-output` <- renderUI({
  `tutorial-exercise-for2-result`()
})
</script>
 
<script type="application/shiny-prerendered" data-context="server">
`tutorial-exercise-sim1-result` <- learnr:::setup_exercise_handler(reactive(req(input$`tutorial-exercise-sim1-code-editor`)), session)
output$`tutorial-exercise-sim1-output` <- renderUI({
  `tutorial-exercise-sim1-result`()
})
</script>
 
<script type="application/shiny-prerendered" data-context="server">
`tutorial-exercise-sim2-result` <- learnr:::setup_exercise_handler(reactive(req(input$`tutorial-exercise-sim2-code-editor`)), session)
output$`tutorial-exercise-sim2-output` <- renderUI({
  `tutorial-exercise-sim2-result`()
})
</script>
 
<script type="application/shiny-prerendered" data-context="server">
`tutorial-exercise-sim3-result` <- learnr:::setup_exercise_handler(reactive(req(input$`tutorial-exercise-sim3-code-editor`)), session)
output$`tutorial-exercise-sim3-output` <- renderUI({
  `tutorial-exercise-sim3-result`()
})
</script>
 
<script type="application/shiny-prerendered" data-context="server">
`tutorial-exercise-medSim-result` <- learnr:::setup_exercise_handler(reactive(req(input$`tutorial-exercise-medSim-code-editor`)), session)
output$`tutorial-exercise-medSim-output` <- renderUI({
  `tutorial-exercise-medSim-result`()
})
</script>
 
<script type="application/shiny-prerendered" data-context="server">
`tutorial-exercise-medBias-result` <- learnr:::setup_exercise_handler(reactive(req(input$`tutorial-exercise-medBias-code-editor`)), session)
output$`tutorial-exercise-medBias-output` <- renderUI({
  `tutorial-exercise-medBias-result`()
})
</script>
 
<script type="application/shiny-prerendered" data-context="server">
learnr:::question_prerendered_chunk(structure(list(type = "learnr_radio", label = "efficient", question = structure("Which is the more efficient estimator of \\(\\mu\\), \\(\\hat{\\mu}_A\\) or \\(\\hat{\\mu}_B\\)?", html = TRUE, class = c("html", "character")), answers = list(structure(list(id = "lnr_ans_33951c3",     option = "$\\hat{\\mu}_A$, the sample mean", value = "$\\hat{\\mu}_A$, the sample mean",     label = structure("\\(\\hat{\\mu}_A\\), the sample mean", html = TRUE, class = c("html",     "character")), correct = TRUE, message = NULL), class = c("tutorial_question_answer", "tutorial_quiz_answer")), structure(list(id = "lnr_ans_fa6aaea",     option = "$\\hat{\\mu}_B$, the sample median", value = "$\\hat{\\mu}_B$, the sample median",     label = structure("\\(\\hat{\\mu}_B\\), the sample median", html = TRUE, class = c("html",     "character")), correct = FALSE, message = NULL), class = c("tutorial_question_answer", "tutorial_quiz_answer"))), button_labels = list(submit = structure("Submit Answer", html = TRUE, class = c("html", "character")), try_again = structure("Try Again", html = TRUE, class = c("html", "character"))), messages = list(correct = structure("Correct!", html = TRUE, class = c("html", "character")), try_again = structure("Incorrect", html = TRUE, class = c("html", "character")), incorrect = structure("Incorrect", html = TRUE, class = c("html", "character")), message = NULL, post_message = NULL), ids = list(    answer = "efficient-answer", question = "efficient"), loading = structure("<strong>Loading:<\u002fstrong> \nWhich is the more efficient estimator of \\(\\mu\\), \\(\\hat{\\mu}_A\\) or \\(\\hat{\\mu}_B\\)?\n<br/><br/><br/>", html = TRUE, class = c("html", "character")), random_answer_order = FALSE, allow_retry = TRUE,     seed = 886293266.087288, options = list()), class = c("learnr_radio", "tutorial_question")))
</script>
 
<script type="application/shiny-prerendered" data-context="server">
`tutorial-exercise-effCalc-result` <- learnr:::setup_exercise_handler(reactive(req(input$`tutorial-exercise-effCalc-code-editor`)), session)
output$`tutorial-exercise-effCalc-output` <- renderUI({
  `tutorial-exercise-effCalc-result`()
})
</script>
 
<script type="application/shiny-prerendered" data-context="server">
learnr:::question_prerendered_chunk(structure(list(type = "learnr_checkbox", label = "ses", question = structure("Which of these lead to smaller standard errors?", html = TRUE, class = c("html", "character")), answers = list(structure(list(id = "lnr_ans_79ae143",     option = "A more efficient estimator", value = "A more efficient estimator",     label = structure("A more efficient estimator", html = TRUE, class = c("html",     "character")), correct = TRUE, message = NULL), class = c("tutorial_question_answer", "tutorial_quiz_answer")), structure(list(id = "lnr_ans_488ab4",     option = "A less efficient estimator", value = "A less efficient estimator",     label = structure("A less efficient estimator", html = TRUE, class = c("html",     "character")), correct = FALSE, message = NULL), class = c("tutorial_question_answer", "tutorial_quiz_answer")), structure(list(id = "lnr_ans_5e3a5c8",     option = "A larger sample size", value = "A larger sample size",     label = structure("A larger sample size", html = TRUE, class = c("html",     "character")), correct = TRUE, message = NULL), class = c("tutorial_question_answer", "tutorial_quiz_answer")), structure(list(id = "lnr_ans_c348ee",     option = "A smaller sample size", value = "A smaller sample size",     label = structure("A smaller sample size", html = TRUE, class = c("html",     "character")), correct = FALSE, message = NULL), class = c("tutorial_question_answer", "tutorial_quiz_answer")), structure(list(id = "lnr_ans_75762de",     option = "A random variable with more variability", value = "A random variable with more variability",     label = structure("A random variable with more variability", html = TRUE, class = c("html",     "character")), correct = FALSE, message = NULL), class = c("tutorial_question_answer", "tutorial_quiz_answer")), structure(list(id = "lnr_ans_22d820e",     option = "A random variable with less variability", value = "A random variable with less variability",     label = structure("A random variable with less variability", html = TRUE, class = c("html",     "character")), correct = TRUE, message = NULL), class = c("tutorial_question_answer", "tutorial_quiz_answer"))), button_labels = list(submit = structure("Submit Answer", html = TRUE, class = c("html", "character")), try_again = structure("Try Again", html = TRUE, class = c("html", "character"))), messages = list(correct = structure("Correct!", html = TRUE, class = c("html", "character")), try_again = structure("Incorrect", html = TRUE, class = c("html", "character")), incorrect = structure("Incorrect", html = TRUE, class = c("html", "character")), message = NULL, post_message = NULL), ids = list(    answer = "ses-answer", question = "ses"), loading = structure("<strong>Loading:<\u002fstrong> \nWhich of these lead to smaller standard errors?\n<br/><br/><br/>", html = TRUE, class = c("html", "character")), random_answer_order = FALSE, allow_retry = TRUE,     seed = 1034096473.51846, options = list()), class = c("learnr_checkbox", "tutorial_question")))
</script>
 
<script type="application/shiny-prerendered" data-context="server">
`tutorial-exercise-nComp-result` <- learnr:::setup_exercise_handler(reactive(req(input$`tutorial-exercise-nComp-code-editor`)), session)
output$`tutorial-exercise-nComp-output` <- renderUI({
  `tutorial-exercise-nComp-result`()
})
</script>
 
<script type="application/shiny-prerendered" data-context="server">
`tutorial-exercise-varComp-result` <- learnr:::setup_exercise_handler(reactive(req(input$`tutorial-exercise-varComp-code-editor`)), session)
output$`tutorial-exercise-varComp-output` <- renderUI({
  `tutorial-exercise-varComp-result`()
})
</script>
 
<script type="application/shiny-prerendered" data-context="server">
learnr:::question_prerendered_chunk(structure(list(type = "learnr_radio", label = "TF1", question = structure("Increasing \\(n\\) would make the distribution of \\(X\\) narrower.", html = TRUE, class = c("html", "character")), answers = list(structure(list(id = "lnr_ans_c42cf8",     option = "True", value = "True", label = structure("True", html = TRUE, class = c("html",     "character")), correct = FALSE, message = structure("The distribution of \\(X\\) is what it is: it doesn&#39;t matter how many samples you take of it.", html = TRUE, class = c("html",     "character"))), class = c("tutorial_question_answer", "tutorial_quiz_answer")), structure(list(id = "lnr_ans_3b9d759", option = "False",     value = "False", label = structure("False", html = TRUE, class = c("html",     "character")), correct = TRUE, message = NULL), class = c("tutorial_question_answer", "tutorial_quiz_answer"))), button_labels = list(submit = structure("Submit Answer", html = TRUE, class = c("html", "character")), try_again = structure("Try Again", html = TRUE, class = c("html", "character"))), messages = list(correct = structure("Correct!", html = TRUE, class = c("html", "character")), try_again = structure("Incorrect", html = TRUE, class = c("html", "character")), incorrect = structure("Incorrect", html = TRUE, class = c("html", "character")), message = NULL, post_message = NULL), ids = list(    answer = "TF1-answer", question = "TF1"), loading = structure("<strong>Loading:<\u002fstrong> \nIncreasing \\(n\\) would make the distribution of \\(X\\) narrower.\n<br/><br/><br/>", html = TRUE, class = c("html", "character")), random_answer_order = FALSE, allow_retry = TRUE,     seed = 1031350856.01974, options = list()), class = c("learnr_radio", "tutorial_question")))
</script>
 
<script type="application/shiny-prerendered" data-context="server">
learnr:::question_prerendered_chunk(structure(list(type = "learnr_radio", label = "TF2", question = structure("Increasing \\(n\\) would make the distribution of \\(\\bar{X}\\) narrower.", html = TRUE, class = c("html", "character")), answers = list(structure(list(id = "lnr_ans_9931334",     option = "True", value = "True", label = structure("True", html = TRUE, class = c("html",     "character")), correct = TRUE, message = NULL), class = c("tutorial_question_answer", "tutorial_quiz_answer")), structure(list(id = "lnr_ans_f0c7134",     option = "False", value = "False", label = structure("False", html = TRUE, class = c("html",     "character")), correct = FALSE, message = NULL), class = c("tutorial_question_answer", "tutorial_quiz_answer"))), button_labels = list(submit = structure("Submit Answer", html = TRUE, class = c("html", "character")), try_again = structure("Try Again", html = TRUE, class = c("html", "character"))), messages = list(correct = structure("Correct!", html = TRUE, class = c("html", "character")), try_again = structure("Incorrect", html = TRUE, class = c("html", "character")), incorrect = structure("Incorrect", html = TRUE, class = c("html", "character")), message = NULL, post_message = NULL), ids = list(    answer = "TF2-answer", question = "TF2"), loading = structure("<strong>Loading:<\u002fstrong> \nIncreasing \\(n\\) would make the distribution of \\(\\bar{X}\\) narrower.\n<br/><br/><br/>", html = TRUE, class = c("html", "character")), random_answer_order = FALSE, allow_retry = TRUE,     seed = 1109873957.98317, options = list()), class = c("learnr_radio", "tutorial_question")))
</script>
 
<script type="application/shiny-prerendered" data-context="server">
learnr:::question_prerendered_chunk(structure(list(type = "learnr_radio", label = "cltPois", question = structure("What&#39;s the standard error of the sample mean of 100 independent samples of \\(X\\), where \\(X \\sim Poisson(9)\\) (which for the Poisson means its mean and variance are both 9)?", html = TRUE, class = c("html", "character")), answers = list(structure(list(id = "lnr_ans_57c6c93",     option = "0.03", value = "0.03", label = structure("0.03", html = TRUE, class = c("html",     "character")), correct = FALSE, message = NULL), class = c("tutorial_question_answer", "tutorial_quiz_answer")), structure(list(id = "lnr_ans_222f346",     option = "0.3", value = "0.3", label = structure("0.3", html = TRUE, class = c("html",     "character")), correct = TRUE, message = NULL), class = c("tutorial_question_answer", "tutorial_quiz_answer")), structure(list(id = "lnr_ans_c8b9113",     option = "0.09", value = "0.09", label = structure("0.09", html = TRUE, class = c("html",     "character")), correct = FALSE, message = NULL), class = c("tutorial_question_answer", "tutorial_quiz_answer")), structure(list(id = "lnr_ans_a90c4ff",     option = "0.9", value = "0.9", label = structure("0.9", html = TRUE, class = c("html",     "character")), correct = FALSE, message = NULL), class = c("tutorial_question_answer", "tutorial_quiz_answer")), structure(list(id = "lnr_ans_c524568",     option = "Not enough information", value = "Not enough information",     label = structure("Not enough information", html = TRUE, class = c("html",     "character")), correct = FALSE, message = NULL), class = c("tutorial_question_answer", "tutorial_quiz_answer"))), button_labels = list(submit = structure("Submit Answer", html = TRUE, class = c("html", "character")), try_again = structure("Try Again", html = TRUE, class = c("html", "character"))), messages = list(correct = structure("Correct!", html = TRUE, class = c("html", "character")), try_again = structure("Incorrect", html = TRUE, class = c("html", "character")), incorrect = structure("Incorrect", html = TRUE, class = c("html", "character")), message = NULL, post_message = NULL), ids = list(    answer = "cltPois-answer", question = "cltPois"), loading = structure("<strong>Loading:<\u002fstrong> \nWhat&#39;s the standard error of the sample mean of 100 independent samples of \\(X\\), where \\(X \\sim Poisson(9)\\) (which for the Poisson means its mean and variance are both 9)?\n<br/><br/><br/>", html = TRUE, class = c("html", "character")), random_answer_order = FALSE, allow_retry = TRUE,     seed = 55586470.4741155, options = list()), class = c("learnr_radio", "tutorial_question")))
</script>
 
<script type="application/shiny-prerendered" data-context="server">
`tutorial-exercise-PoisSim-result` <- learnr:::setup_exercise_handler(reactive(req(input$`tutorial-exercise-PoisSim-code-editor`)), session)
output$`tutorial-exercise-PoisSim-output` <- renderUI({
  `tutorial-exercise-PoisSim-result`()
})
</script>
 
<script type="application/shiny-prerendered" data-context="server">
`tutorial-exercise-pCalc-result` <- learnr:::setup_exercise_handler(reactive(req(input$`tutorial-exercise-pCalc-code-editor`)), session)
output$`tutorial-exercise-pCalc-output` <- renderUI({
  `tutorial-exercise-pCalc-result`()
})
</script>
 <!--html_preserve-->
<script type="application/shiny-prerendered" data-context="dependencies">
{"type":"list","attributes":{},"value":[{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["name","version","src","meta","script","stylesheet","head","attachment","package","all_files","pkgVersion"]},"class":{"type":"character","attributes":{},"value":["html_dependency"]}},"value":[{"type":"character","attributes":{},"value":["header-attrs"]},{"type":"character","attributes":{},"value":["2.9"]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["file"]}},"value":[{"type":"character","attributes":{},"value":["rmd/h/pandoc"]}]},{"type":"NULL"},{"type":"character","attributes":{},"value":["header-attrs.js"]},{"type":"NULL"},{"type":"NULL"},{"type":"NULL"},{"type":"character","attributes":{},"value":["rmarkdown"]},{"type":"logical","attributes":{},"value":[true]},{"type":"character","attributes":{},"value":["2.9"]}]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["name","version","src","meta","script","stylesheet","head","attachment","package","all_files","pkgVersion"]},"class":{"type":"character","attributes":{},"value":["html_dependency"]}},"value":[{"type":"character","attributes":{},"value":["jquery"]},{"type":"character","attributes":{},"value":["1.11.3"]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["file"]}},"value":[{"type":"character","attributes":{},"value":["rmd/h/jquery"]}]},{"type":"NULL"},{"type":"character","attributes":{},"value":["jquery.min.js"]},{"type":"NULL"},{"type":"NULL"},{"type":"NULL"},{"type":"character","attributes":{},"value":["rmarkdown"]},{"type":"logical","attributes":{},"value":[true]},{"type":"character","attributes":{},"value":["2.9"]}]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["name","version","src","meta","script","stylesheet","head","attachment","package","all_files","pkgVersion"]},"class":{"type":"character","attributes":{},"value":["html_dependency"]}},"value":[{"type":"character","attributes":{},"value":["bootstrap"]},{"type":"character","attributes":{},"value":["3.3.5"]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["file"]}},"value":[{"type":"character","attributes":{},"value":["rmd/h/bootstrap"]}]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["viewport"]}},"value":[{"type":"character","attributes":{},"value":["width=device-width, initial-scale=1"]}]},{"type":"character","attributes":{},"value":["js/bootstrap.min.js","shim/html5shiv.min.js","shim/respond.min.js"]},{"type":"character","attributes":{},"value":["css/paper.min.css"]},{"type":"character","attributes":{},"value":["<style>h1 {font-size: 34px;}\n       h1.title {font-size: 38px;}\n       h2 {font-size: 30px;}\n       h3 {font-size: 24px;}\n       h4 {font-size: 18px;}\n       h5 {font-size: 16px;}\n       h6 {font-size: 12px;}\n       code {color: inherit; background-color: rgba(0, 0, 0, 0.04);}\n       pre:not([class]) { background-color: white }<\/style>"]},{"type":"NULL"},{"type":"character","attributes":{},"value":["rmarkdown"]},{"type":"logical","attributes":{},"value":[true]},{"type":"character","attributes":{},"value":["2.9"]}]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["name","version","src","meta","script","stylesheet","head","attachment","package","all_files","pkgVersion"]},"class":{"type":"character","attributes":{},"value":["html_dependency"]}},"value":[{"type":"character","attributes":{},"value":["pagedtable"]},{"type":"character","attributes":{},"value":["1.1"]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["file"]}},"value":[{"type":"character","attributes":{},"value":["rmd/h/pagedtable-1.1"]}]},{"type":"NULL"},{"type":"character","attributes":{},"value":["js/pagedtable.js"]},{"type":"character","attributes":{},"value":["css/pagedtable.css"]},{"type":"NULL"},{"type":"NULL"},{"type":"character","attributes":{},"value":["rmarkdown"]},{"type":"logical","attributes":{},"value":[true]},{"type":"character","attributes":{},"value":["2.9"]}]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["name","version","src","meta","script","stylesheet","head","attachment","package","all_files","pkgVersion"]},"class":{"type":"character","attributes":{},"value":["html_dependency"]}},"value":[{"type":"character","attributes":{},"value":["highlightjs"]},{"type":"character","attributes":{},"value":["9.12.0"]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["file"]}},"value":[{"type":"character","attributes":{},"value":["rmd/h/highlightjs"]}]},{"type":"NULL"},{"type":"character","attributes":{},"value":["highlight.js"]},{"type":"character","attributes":{},"value":["textmate.css"]},{"type":"NULL"},{"type":"NULL"},{"type":"character","attributes":{},"value":["rmarkdown"]},{"type":"logical","attributes":{},"value":[true]},{"type":"character","attributes":{},"value":["2.9"]}]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["name","version","src","meta","script","stylesheet","head","attachment","package","all_files","pkgVersion"]},"class":{"type":"character","attributes":{},"value":["html_dependency"]}},"value":[{"type":"character","attributes":{},"value":["tutorial"]},{"type":"character","attributes":{},"value":["0.10.1"]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["file"]}},"value":[{"type":"character","attributes":{},"value":["lib/tutorial"]}]},{"type":"NULL"},{"type":"character","attributes":{},"value":["tutorial.js"]},{"type":"character","attributes":{},"value":["tutorial.css"]},{"type":"NULL"},{"type":"NULL"},{"type":"character","attributes":{},"value":["learnr"]},{"type":"logical","attributes":{},"value":[true]},{"type":"character","attributes":{},"value":["0.10.1"]}]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["name","version","src","meta","script","stylesheet","head","attachment","package","all_files","pkgVersion"]},"class":{"type":"character","attributes":{},"value":["html_dependency"]}},"value":[{"type":"character","attributes":{},"value":["tutorial-autocompletion"]},{"type":"character","attributes":{},"value":["0.10.1"]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["file"]}},"value":[{"type":"character","attributes":{},"value":["lib/tutorial"]}]},{"type":"NULL"},{"type":"character","attributes":{},"value":["tutorial-autocompletion.js"]},{"type":"NULL"},{"type":"NULL"},{"type":"NULL"},{"type":"character","attributes":{},"value":["learnr"]},{"type":"logical","attributes":{},"value":[true]},{"type":"character","attributes":{},"value":["0.10.1"]}]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["name","version","src","meta","script","stylesheet","head","attachment","package","all_files","pkgVersion"]},"class":{"type":"character","attributes":{},"value":["html_dependency"]}},"value":[{"type":"character","attributes":{},"value":["tutorial-diagnostics"]},{"type":"character","attributes":{},"value":["0.10.1"]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["file"]}},"value":[{"type":"character","attributes":{},"value":["lib/tutorial"]}]},{"type":"NULL"},{"type":"character","attributes":{},"value":["tutorial-diagnostics.js"]},{"type":"NULL"},{"type":"NULL"},{"type":"NULL"},{"type":"character","attributes":{},"value":["learnr"]},{"type":"logical","attributes":{},"value":[true]},{"type":"character","attributes":{},"value":["0.10.1"]}]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["name","version","src","meta","script","stylesheet","head","attachment","package","all_files","pkgVersion"]},"class":{"type":"character","attributes":{},"value":["html_dependency"]}},"value":[{"type":"character","attributes":{},"value":["tutorial-format"]},{"type":"character","attributes":{},"value":["0.10.1"]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["file"]}},"value":[{"type":"character","attributes":{},"value":["rmarkdown/templates/tutorial/resources"]}]},{"type":"NULL"},{"type":"character","attributes":{},"value":["tutorial-format.js"]},{"type":"character","attributes":{},"value":["tutorial-format.css"]},{"type":"NULL"},{"type":"NULL"},{"type":"character","attributes":{},"value":["learnr"]},{"type":"logical","attributes":{},"value":[true]},{"type":"character","attributes":{},"value":["0.10.1"]}]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["name","version","src","meta","script","stylesheet","head","attachment","package","all_files","pkgVersion"]},"class":{"type":"character","attributes":{},"value":["html_dependency"]}},"value":[{"type":"character","attributes":{},"value":["jquery"]},{"type":"character","attributes":{},"value":["1.11.3"]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["file"]}},"value":[{"type":"character","attributes":{},"value":["rmd/h/jquery"]}]},{"type":"NULL"},{"type":"character","attributes":{},"value":["jquery.min.js"]},{"type":"NULL"},{"type":"NULL"},{"type":"NULL"},{"type":"character","attributes":{},"value":["rmarkdown"]},{"type":"logical","attributes":{},"value":[true]},{"type":"character","attributes":{},"value":["2.9"]}]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["name","version","src","meta","script","stylesheet","head","attachment","package","all_files","pkgVersion"]},"class":{"type":"character","attributes":{},"value":["html_dependency"]}},"value":[{"type":"character","attributes":{},"value":["navigation"]},{"type":"character","attributes":{},"value":["1.1"]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["file"]}},"value":[{"type":"character","attributes":{},"value":["rmd/h/navigation-1.1"]}]},{"type":"NULL"},{"type":"character","attributes":{},"value":["tabsets.js"]},{"type":"NULL"},{"type":"NULL"},{"type":"NULL"},{"type":"character","attributes":{},"value":["rmarkdown"]},{"type":"logical","attributes":{},"value":[true]},{"type":"character","attributes":{},"value":["2.9"]}]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["name","version","src","meta","script","stylesheet","head","attachment","package","all_files","pkgVersion"]},"class":{"type":"character","attributes":{},"value":["html_dependency"]}},"value":[{"type":"character","attributes":{},"value":["highlightjs"]},{"type":"character","attributes":{},"value":["9.12.0"]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["file"]}},"value":[{"type":"character","attributes":{},"value":["rmd/h/highlightjs"]}]},{"type":"NULL"},{"type":"character","attributes":{},"value":["highlight.js"]},{"type":"character","attributes":{},"value":["default.css"]},{"type":"NULL"},{"type":"NULL"},{"type":"character","attributes":{},"value":["rmarkdown"]},{"type":"logical","attributes":{},"value":[true]},{"type":"character","attributes":{},"value":["2.9"]}]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["name","version","src","meta","script","stylesheet","head","attachment","package","all_files","pkgVersion"]},"class":{"type":"character","attributes":{},"value":["html_dependency"]}},"value":[{"type":"character","attributes":{},"value":["jquery"]},{"type":"character","attributes":{},"value":["1.11.3"]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["file"]}},"value":[{"type":"character","attributes":{},"value":["rmd/h/jquery"]}]},{"type":"NULL"},{"type":"character","attributes":{},"value":["jquery.min.js"]},{"type":"NULL"},{"type":"NULL"},{"type":"NULL"},{"type":"character","attributes":{},"value":["rmarkdown"]},{"type":"logical","attributes":{},"value":[true]},{"type":"character","attributes":{},"value":["2.9"]}]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["name","version","src","meta","script","stylesheet","head","attachment","package","all_files","pkgVersion"]},"class":{"type":"character","attributes":{},"value":["html_dependency"]}},"value":[{"type":"character","attributes":{},"value":["font-awesome"]},{"type":"character","attributes":{},"value":["5.1.0"]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["file"]}},"value":[{"type":"character","attributes":{},"value":["rmd/h/fontawesome"]}]},{"type":"NULL"},{"type":"NULL"},{"type":"character","attributes":{},"value":["css/all.css","css/v4-shims.css"]},{"type":"NULL"},{"type":"NULL"},{"type":"character","attributes":{},"value":["rmarkdown"]},{"type":"logical","attributes":{},"value":[true]},{"type":"character","attributes":{},"value":["2.9"]}]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["name","version","src","meta","script","stylesheet","head","attachment","package","all_files","pkgVersion"]},"class":{"type":"character","attributes":{},"value":["html_dependency"]}},"value":[{"type":"character","attributes":{},"value":["bootbox"]},{"type":"character","attributes":{},"value":["4.4.0"]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["file"]}},"value":[{"type":"character","attributes":{},"value":["lib/bootbox"]}]},{"type":"NULL"},{"type":"character","attributes":{},"value":["bootbox.min.js"]},{"type":"NULL"},{"type":"NULL"},{"type":"NULL"},{"type":"character","attributes":{},"value":["learnr"]},{"type":"logical","attributes":{},"value":[true]},{"type":"character","attributes":{},"value":["0.10.1"]}]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["name","version","src","meta","script","stylesheet","head","attachment","package","all_files","pkgVersion"]},"class":{"type":"character","attributes":{},"value":["html_dependency"]}},"value":[{"type":"character","attributes":{},"value":["idb-keyvalue"]},{"type":"character","attributes":{},"value":["3.2.0"]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["file"]}},"value":[{"type":"character","attributes":{},"value":["lib/idb-keyval"]}]},{"type":"NULL"},{"type":"character","attributes":{},"value":["idb-keyval-iife-compat.min.js"]},{"type":"NULL"},{"type":"NULL"},{"type":"NULL"},{"type":"character","attributes":{},"value":["learnr"]},{"type":"logical","attributes":{},"value":[false]},{"type":"character","attributes":{},"value":["0.10.1"]}]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["name","version","src","meta","script","stylesheet","head","attachment","package","all_files","pkgVersion"]},"class":{"type":"character","attributes":{},"value":["html_dependency"]}},"value":[{"type":"character","attributes":{},"value":["tutorial"]},{"type":"character","attributes":{},"value":["0.10.1"]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["file"]}},"value":[{"type":"character","attributes":{},"value":["lib/tutorial"]}]},{"type":"NULL"},{"type":"character","attributes":{},"value":["tutorial.js"]},{"type":"character","attributes":{},"value":["tutorial.css"]},{"type":"NULL"},{"type":"NULL"},{"type":"character","attributes":{},"value":["learnr"]},{"type":"logical","attributes":{},"value":[true]},{"type":"character","attributes":{},"value":["0.10.1"]}]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["name","version","src","meta","script","stylesheet","head","attachment","package","all_files","pkgVersion"]},"class":{"type":"character","attributes":{},"value":["html_dependency"]}},"value":[{"type":"character","attributes":{},"value":["tutorial-autocompletion"]},{"type":"character","attributes":{},"value":["0.10.1"]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["file"]}},"value":[{"type":"character","attributes":{},"value":["lib/tutorial"]}]},{"type":"NULL"},{"type":"character","attributes":{},"value":["tutorial-autocompletion.js"]},{"type":"NULL"},{"type":"NULL"},{"type":"NULL"},{"type":"character","attributes":{},"value":["learnr"]},{"type":"logical","attributes":{},"value":[true]},{"type":"character","attributes":{},"value":["0.10.1"]}]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["name","version","src","meta","script","stylesheet","head","attachment","package","all_files","pkgVersion"]},"class":{"type":"character","attributes":{},"value":["html_dependency"]}},"value":[{"type":"character","attributes":{},"value":["tutorial-diagnostics"]},{"type":"character","attributes":{},"value":["0.10.1"]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["file"]}},"value":[{"type":"character","attributes":{},"value":["lib/tutorial"]}]},{"type":"NULL"},{"type":"character","attributes":{},"value":["tutorial-diagnostics.js"]},{"type":"NULL"},{"type":"NULL"},{"type":"NULL"},{"type":"character","attributes":{},"value":["learnr"]},{"type":"logical","attributes":{},"value":[true]},{"type":"character","attributes":{},"value":["0.10.1"]}]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["name","version","src","meta","script","stylesheet","head","attachment","package","all_files","pkgVersion"]},"class":{"type":"character","attributes":{},"value":["html_dependency"]}},"value":[{"type":"character","attributes":{},"value":["ace"]},{"type":"character","attributes":{},"value":["1.2.6"]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["file"]}},"value":[{"type":"character","attributes":{},"value":["lib/ace"]}]},{"type":"NULL"},{"type":"character","attributes":{},"value":["ace.js"]},{"type":"NULL"},{"type":"NULL"},{"type":"NULL"},{"type":"character","attributes":{},"value":["learnr"]},{"type":"logical","attributes":{},"value":[true]},{"type":"character","attributes":{},"value":["0.10.1"]}]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["name","version","src","meta","script","stylesheet","head","attachment","package","all_files","pkgVersion"]},"class":{"type":"character","attributes":{},"value":["html_dependency"]}},"value":[{"type":"character","attributes":{},"value":["clipboardjs"]},{"type":"character","attributes":{},"value":["1.5.15"]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["file"]}},"value":[{"type":"character","attributes":{},"value":["lib/clipboardjs"]}]},{"type":"NULL"},{"type":"character","attributes":{},"value":["clipboard.min.js"]},{"type":"NULL"},{"type":"NULL"},{"type":"NULL"},{"type":"character","attributes":{},"value":["learnr"]},{"type":"logical","attributes":{},"value":[true]},{"type":"character","attributes":{},"value":["0.10.1"]}]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["name","version","src","meta","script","stylesheet","head","attachment","package","all_files","pkgVersion"]},"class":{"type":"character","attributes":{},"value":["html_dependency"]}},"value":[{"type":"character","attributes":{},"value":["ace"]},{"type":"character","attributes":{},"value":["1.2.6"]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["file"]}},"value":[{"type":"character","attributes":{},"value":["lib/ace"]}]},{"type":"NULL"},{"type":"character","attributes":{},"value":["ace.js"]},{"type":"NULL"},{"type":"NULL"},{"type":"NULL"},{"type":"character","attributes":{},"value":["learnr"]},{"type":"logical","attributes":{},"value":[true]},{"type":"character","attributes":{},"value":["0.10.1"]}]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["name","version","src","meta","script","stylesheet","head","attachment","package","all_files","pkgVersion"]},"class":{"type":"character","attributes":{},"value":["html_dependency"]}},"value":[{"type":"character","attributes":{},"value":["clipboardjs"]},{"type":"character","attributes":{},"value":["1.5.15"]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["file"]}},"value":[{"type":"character","attributes":{},"value":["lib/clipboardjs"]}]},{"type":"NULL"},{"type":"character","attributes":{},"value":["clipboard.min.js"]},{"type":"NULL"},{"type":"NULL"},{"type":"NULL"},{"type":"character","attributes":{},"value":["learnr"]},{"type":"logical","attributes":{},"value":[true]},{"type":"character","attributes":{},"value":["0.10.1"]}]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["name","version","src","meta","script","stylesheet","head","attachment","package","all_files","pkgVersion"]},"class":{"type":"character","attributes":{},"value":["html_dependency"]}},"value":[{"type":"character","attributes":{},"value":["ace"]},{"type":"character","attributes":{},"value":["1.2.6"]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["file"]}},"value":[{"type":"character","attributes":{},"value":["lib/ace"]}]},{"type":"NULL"},{"type":"character","attributes":{},"value":["ace.js"]},{"type":"NULL"},{"type":"NULL"},{"type":"NULL"},{"type":"character","attributes":{},"value":["learnr"]},{"type":"logical","attributes":{},"value":[true]},{"type":"character","attributes":{},"value":["0.10.1"]}]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["name","version","src","meta","script","stylesheet","head","attachment","package","all_files","pkgVersion"]},"class":{"type":"character","attributes":{},"value":["html_dependency"]}},"value":[{"type":"character","attributes":{},"value":["clipboardjs"]},{"type":"character","attributes":{},"value":["1.5.15"]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["file"]}},"value":[{"type":"character","attributes":{},"value":["lib/clipboardjs"]}]},{"type":"NULL"},{"type":"character","attributes":{},"value":["clipboard.min.js"]},{"type":"NULL"},{"type":"NULL"},{"type":"NULL"},{"type":"character","attributes":{},"value":["learnr"]},{"type":"logical","attributes":{},"value":[true]},{"type":"character","attributes":{},"value":["0.10.1"]}]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["name","version","src","meta","script","stylesheet","head","attachment","package","all_files","pkgVersion"]},"class":{"type":"character","attributes":{},"value":["html_dependency"]}},"value":[{"type":"character","attributes":{},"value":["ace"]},{"type":"character","attributes":{},"value":["1.2.6"]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["file"]}},"value":[{"type":"character","attributes":{},"value":["lib/ace"]}]},{"type":"NULL"},{"type":"character","attributes":{},"value":["ace.js"]},{"type":"NULL"},{"type":"NULL"},{"type":"NULL"},{"type":"character","attributes":{},"value":["learnr"]},{"type":"logical","attributes":{},"value":[true]},{"type":"character","attributes":{},"value":["0.10.1"]}]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["name","version","src","meta","script","stylesheet","head","attachment","package","all_files","pkgVersion"]},"class":{"type":"character","attributes":{},"value":["html_dependency"]}},"value":[{"type":"character","attributes":{},"value":["clipboardjs"]},{"type":"character","attributes":{},"value":["1.5.15"]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["file"]}},"value":[{"type":"character","attributes":{},"value":["lib/clipboardjs"]}]},{"type":"NULL"},{"type":"character","attributes":{},"value":["clipboard.min.js"]},{"type":"NULL"},{"type":"NULL"},{"type":"NULL"},{"type":"character","attributes":{},"value":["learnr"]},{"type":"logical","attributes":{},"value":[true]},{"type":"character","attributes":{},"value":["0.10.1"]}]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["name","version","src","meta","script","stylesheet","head","attachment","package","all_files","pkgVersion"]},"class":{"type":"character","attributes":{},"value":["html_dependency"]}},"value":[{"type":"character","attributes":{},"value":["ace"]},{"type":"character","attributes":{},"value":["1.2.6"]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["file"]}},"value":[{"type":"character","attributes":{},"value":["lib/ace"]}]},{"type":"NULL"},{"type":"character","attributes":{},"value":["ace.js"]},{"type":"NULL"},{"type":"NULL"},{"type":"NULL"},{"type":"character","attributes":{},"value":["learnr"]},{"type":"logical","attributes":{},"value":[true]},{"type":"character","attributes":{},"value":["0.10.1"]}]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["name","version","src","meta","script","stylesheet","head","attachment","package","all_files","pkgVersion"]},"class":{"type":"character","attributes":{},"value":["html_dependency"]}},"value":[{"type":"character","attributes":{},"value":["clipboardjs"]},{"type":"character","attributes":{},"value":["1.5.15"]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["file"]}},"value":[{"type":"character","attributes":{},"value":["lib/clipboardjs"]}]},{"type":"NULL"},{"type":"character","attributes":{},"value":["clipboard.min.js"]},{"type":"NULL"},{"type":"NULL"},{"type":"NULL"},{"type":"character","attributes":{},"value":["learnr"]},{"type":"logical","attributes":{},"value":[true]},{"type":"character","attributes":{},"value":["0.10.1"]}]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["name","version","src","meta","script","stylesheet","head","attachment","package","all_files","pkgVersion"]},"class":{"type":"character","attributes":{},"value":["html_dependency"]}},"value":[{"type":"character","attributes":{},"value":["ace"]},{"type":"character","attributes":{},"value":["1.2.6"]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["file"]}},"value":[{"type":"character","attributes":{},"value":["lib/ace"]}]},{"type":"NULL"},{"type":"character","attributes":{},"value":["ace.js"]},{"type":"NULL"},{"type":"NULL"},{"type":"NULL"},{"type":"character","attributes":{},"value":["learnr"]},{"type":"logical","attributes":{},"value":[true]},{"type":"character","attributes":{},"value":["0.10.1"]}]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["name","version","src","meta","script","stylesheet","head","attachment","package","all_files","pkgVersion"]},"class":{"type":"character","attributes":{},"value":["html_dependency"]}},"value":[{"type":"character","attributes":{},"value":["clipboardjs"]},{"type":"character","attributes":{},"value":["1.5.15"]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["file"]}},"value":[{"type":"character","attributes":{},"value":["lib/clipboardjs"]}]},{"type":"NULL"},{"type":"character","attributes":{},"value":["clipboard.min.js"]},{"type":"NULL"},{"type":"NULL"},{"type":"NULL"},{"type":"character","attributes":{},"value":["learnr"]},{"type":"logical","attributes":{},"value":[true]},{"type":"character","attributes":{},"value":["0.10.1"]}]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["name","version","src","meta","script","stylesheet","head","attachment","package","all_files","pkgVersion"]},"class":{"type":"character","attributes":{},"value":["html_dependency"]}},"value":[{"type":"character","attributes":{},"value":["ace"]},{"type":"character","attributes":{},"value":["1.2.6"]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["file"]}},"value":[{"type":"character","attributes":{},"value":["lib/ace"]}]},{"type":"NULL"},{"type":"character","attributes":{},"value":["ace.js"]},{"type":"NULL"},{"type":"NULL"},{"type":"NULL"},{"type":"character","attributes":{},"value":["learnr"]},{"type":"logical","attributes":{},"value":[true]},{"type":"character","attributes":{},"value":["0.10.1"]}]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["name","version","src","meta","script","stylesheet","head","attachment","package","all_files","pkgVersion"]},"class":{"type":"character","attributes":{},"value":["html_dependency"]}},"value":[{"type":"character","attributes":{},"value":["clipboardjs"]},{"type":"character","attributes":{},"value":["1.5.15"]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["file"]}},"value":[{"type":"character","attributes":{},"value":["lib/clipboardjs"]}]},{"type":"NULL"},{"type":"character","attributes":{},"value":["clipboard.min.js"]},{"type":"NULL"},{"type":"NULL"},{"type":"NULL"},{"type":"character","attributes":{},"value":["learnr"]},{"type":"logical","attributes":{},"value":[true]},{"type":"character","attributes":{},"value":["0.10.1"]}]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["name","version","src","meta","script","stylesheet","head","attachment","package","all_files","pkgVersion"]},"class":{"type":"character","attributes":{},"value":["html_dependency"]}},"value":[{"type":"character","attributes":{},"value":["ace"]},{"type":"character","attributes":{},"value":["1.2.6"]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["file"]}},"value":[{"type":"character","attributes":{},"value":["lib/ace"]}]},{"type":"NULL"},{"type":"character","attributes":{},"value":["ace.js"]},{"type":"NULL"},{"type":"NULL"},{"type":"NULL"},{"type":"character","attributes":{},"value":["learnr"]},{"type":"logical","attributes":{},"value":[true]},{"type":"character","attributes":{},"value":["0.10.1"]}]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["name","version","src","meta","script","stylesheet","head","attachment","package","all_files","pkgVersion"]},"class":{"type":"character","attributes":{},"value":["html_dependency"]}},"value":[{"type":"character","attributes":{},"value":["clipboardjs"]},{"type":"character","attributes":{},"value":["1.5.15"]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["file"]}},"value":[{"type":"character","attributes":{},"value":["lib/clipboardjs"]}]},{"type":"NULL"},{"type":"character","attributes":{},"value":["clipboard.min.js"]},{"type":"NULL"},{"type":"NULL"},{"type":"NULL"},{"type":"character","attributes":{},"value":["learnr"]},{"type":"logical","attributes":{},"value":[true]},{"type":"character","attributes":{},"value":["0.10.1"]}]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["name","version","src","meta","script","stylesheet","head","attachment","package","all_files","pkgVersion"]},"class":{"type":"character","attributes":{},"value":["html_dependency"]}},"value":[{"type":"character","attributes":{},"value":["ace"]},{"type":"character","attributes":{},"value":["1.2.6"]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["file"]}},"value":[{"type":"character","attributes":{},"value":["lib/ace"]}]},{"type":"NULL"},{"type":"character","attributes":{},"value":["ace.js"]},{"type":"NULL"},{"type":"NULL"},{"type":"NULL"},{"type":"character","attributes":{},"value":["learnr"]},{"type":"logical","attributes":{},"value":[true]},{"type":"character","attributes":{},"value":["0.10.1"]}]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["name","version","src","meta","script","stylesheet","head","attachment","package","all_files","pkgVersion"]},"class":{"type":"character","attributes":{},"value":["html_dependency"]}},"value":[{"type":"character","attributes":{},"value":["clipboardjs"]},{"type":"character","attributes":{},"value":["1.5.15"]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["file"]}},"value":[{"type":"character","attributes":{},"value":["lib/clipboardjs"]}]},{"type":"NULL"},{"type":"character","attributes":{},"value":["clipboard.min.js"]},{"type":"NULL"},{"type":"NULL"},{"type":"NULL"},{"type":"character","attributes":{},"value":["learnr"]},{"type":"logical","attributes":{},"value":[true]},{"type":"character","attributes":{},"value":["0.10.1"]}]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["name","version","src","meta","script","stylesheet","head","attachment","package","all_files","pkgVersion"]},"class":{"type":"character","attributes":{},"value":["html_dependency"]}},"value":[{"type":"character","attributes":{},"value":["ace"]},{"type":"character","attributes":{},"value":["1.2.6"]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["file"]}},"value":[{"type":"character","attributes":{},"value":["lib/ace"]}]},{"type":"NULL"},{"type":"character","attributes":{},"value":["ace.js"]},{"type":"NULL"},{"type":"NULL"},{"type":"NULL"},{"type":"character","attributes":{},"value":["learnr"]},{"type":"logical","attributes":{},"value":[true]},{"type":"character","attributes":{},"value":["0.10.1"]}]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["name","version","src","meta","script","stylesheet","head","attachment","package","all_files","pkgVersion"]},"class":{"type":"character","attributes":{},"value":["html_dependency"]}},"value":[{"type":"character","attributes":{},"value":["clipboardjs"]},{"type":"character","attributes":{},"value":["1.5.15"]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["file"]}},"value":[{"type":"character","attributes":{},"value":["lib/clipboardjs"]}]},{"type":"NULL"},{"type":"character","attributes":{},"value":["clipboard.min.js"]},{"type":"NULL"},{"type":"NULL"},{"type":"NULL"},{"type":"character","attributes":{},"value":["learnr"]},{"type":"logical","attributes":{},"value":[true]},{"type":"character","attributes":{},"value":["0.10.1"]}]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["name","version","src","meta","script","stylesheet","head","attachment","package","all_files","pkgVersion"]},"class":{"type":"character","attributes":{},"value":["html_dependency"]}},"value":[{"type":"character","attributes":{},"value":["ace"]},{"type":"character","attributes":{},"value":["1.2.6"]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["file"]}},"value":[{"type":"character","attributes":{},"value":["lib/ace"]}]},{"type":"NULL"},{"type":"character","attributes":{},"value":["ace.js"]},{"type":"NULL"},{"type":"NULL"},{"type":"NULL"},{"type":"character","attributes":{},"value":["learnr"]},{"type":"logical","attributes":{},"value":[true]},{"type":"character","attributes":{},"value":["0.10.1"]}]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["name","version","src","meta","script","stylesheet","head","attachment","package","all_files","pkgVersion"]},"class":{"type":"character","attributes":{},"value":["html_dependency"]}},"value":[{"type":"character","attributes":{},"value":["clipboardjs"]},{"type":"character","attributes":{},"value":["1.5.15"]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["file"]}},"value":[{"type":"character","attributes":{},"value":["lib/clipboardjs"]}]},{"type":"NULL"},{"type":"character","attributes":{},"value":["clipboard.min.js"]},{"type":"NULL"},{"type":"NULL"},{"type":"NULL"},{"type":"character","attributes":{},"value":["learnr"]},{"type":"logical","attributes":{},"value":[true]},{"type":"character","attributes":{},"value":["0.10.1"]}]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["name","version","src","meta","script","stylesheet","head","attachment","package","all_files","pkgVersion"]},"class":{"type":"character","attributes":{},"value":["html_dependency"]}},"value":[{"type":"character","attributes":{},"value":["ace"]},{"type":"character","attributes":{},"value":["1.2.6"]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["file"]}},"value":[{"type":"character","attributes":{},"value":["lib/ace"]}]},{"type":"NULL"},{"type":"character","attributes":{},"value":["ace.js"]},{"type":"NULL"},{"type":"NULL"},{"type":"NULL"},{"type":"character","attributes":{},"value":["learnr"]},{"type":"logical","attributes":{},"value":[true]},{"type":"character","attributes":{},"value":["0.10.1"]}]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["name","version","src","meta","script","stylesheet","head","attachment","package","all_files","pkgVersion"]},"class":{"type":"character","attributes":{},"value":["html_dependency"]}},"value":[{"type":"character","attributes":{},"value":["clipboardjs"]},{"type":"character","attributes":{},"value":["1.5.15"]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["file"]}},"value":[{"type":"character","attributes":{},"value":["lib/clipboardjs"]}]},{"type":"NULL"},{"type":"character","attributes":{},"value":["clipboard.min.js"]},{"type":"NULL"},{"type":"NULL"},{"type":"NULL"},{"type":"character","attributes":{},"value":["learnr"]},{"type":"logical","attributes":{},"value":[true]},{"type":"character","attributes":{},"value":["0.10.1"]}]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["name","version","src","meta","script","stylesheet","head","attachment","package","all_files","pkgVersion"]},"class":{"type":"character","attributes":{},"value":["html_dependency"]}},"value":[{"type":"character","attributes":{},"value":["ace"]},{"type":"character","attributes":{},"value":["1.2.6"]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["file"]}},"value":[{"type":"character","attributes":{},"value":["lib/ace"]}]},{"type":"NULL"},{"type":"character","attributes":{},"value":["ace.js"]},{"type":"NULL"},{"type":"NULL"},{"type":"NULL"},{"type":"character","attributes":{},"value":["learnr"]},{"type":"logical","attributes":{},"value":[true]},{"type":"character","attributes":{},"value":["0.10.1"]}]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["name","version","src","meta","script","stylesheet","head","attachment","package","all_files","pkgVersion"]},"class":{"type":"character","attributes":{},"value":["html_dependency"]}},"value":[{"type":"character","attributes":{},"value":["clipboardjs"]},{"type":"character","attributes":{},"value":["1.5.15"]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["file"]}},"value":[{"type":"character","attributes":{},"value":["lib/clipboardjs"]}]},{"type":"NULL"},{"type":"character","attributes":{},"value":["clipboard.min.js"]},{"type":"NULL"},{"type":"NULL"},{"type":"NULL"},{"type":"character","attributes":{},"value":["learnr"]},{"type":"logical","attributes":{},"value":[true]},{"type":"character","attributes":{},"value":["0.10.1"]}]}]}
</script>
<!--/html_preserve-->
<!--html_preserve-->
<script type="application/shiny-prerendered" data-context="execution_dependencies">
{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["packages"]}},"value":[{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["packages","version"]},"class":{"type":"character","attributes":{},"value":["data.frame"]},"row.names":{"type":"integer","attributes":{},"value":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45]}},"value":[{"type":"character","attributes":{},"value":["backports","base","bslib","checkmate","codetools","compiler","datasets","digest","ellipsis","evaluate","fastmap","graphics","grDevices","highr","htmltools","htmlwidgets","httpuv","jquerylib","jsonlite","knitr","later","learnr","lifecycle","magrittr","markdown","methods","mime","promises","R6","Rcpp","renv","rlang","rmarkdown","rprojroot","sass","shiny","stats","stringi","stringr","tools","utils","withr","xfun","xtable","yaml"]},{"type":"character","attributes":{},"value":["1.2.1","4.1.0","0.2.5.1","2.0.0","0.2-18","4.1.0","4.1.0","0.6.27","0.3.2","0.14","1.1.0","4.1.0","4.1.0","0.9","0.5.1.1","1.5.3","1.6.1","0.1.4","1.7.2","1.33","1.2.0","0.10.1","1.0.0","2.0.1","1.1","4.1.0","0.11","1.2.0.1","2.5.0","1.0.7","0.14.0","0.4.11","2.9","2.0.2","0.4.0","1.6.0","4.1.0","1.7.3","1.4.0","4.1.0","4.1.0","2.4.2","0.24","1.8-4","2.2.1"]}]}]}
</script>
<!--/html_preserve-->
</div>

</div> <!-- topics -->

<div class="topicsContainer">
<div class="topicsPositioner">
<div class="band">
<div class="bandContent topicsListContainer">

<!-- begin doc-metadata -->
<div id="doc-metadata">
<h2 class="title toc-ignore" style="display:none;">PHS Summer Prep</h2>
</div>
<!-- end doc-metadata -->

</div> <!-- bandContent.topicsListContainer -->
</div> <!-- band -->
</div> <!-- topicsPositioner -->
</div> <!-- topicsContainer -->


</div> <!-- bandContent page -->
</div> <!-- pageContent band -->




<script>
// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.header').parent('thead').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});
</script>


<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>


</body>

</html>
